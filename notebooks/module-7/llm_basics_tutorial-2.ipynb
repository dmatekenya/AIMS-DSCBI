{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Large Language Models (LLMs)\n",
    "## A Beginner's Guide to Next Token Prediction, Tokenization, and Embeddings\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand how LLMs predict the next token\n",
    "2. Learn about tokenization and how it works across different languages\n",
    "3. Build intuition about vector embeddings and how meaning is represented\n",
    "4. Explore vector databases and semantic search for document retrieval\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python knowledge\n",
    "- Understanding of basic machine learning concepts (helpful but not required)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required libraries. We'll use open-source models from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch numpy matplotlib seaborn scikit-learn tokenizers sentencepiece chromadb sentence-transformers --quiet\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Next Token Prediction - The Core of LLMs\n",
    "\n",
    "## What is Next Token Prediction?\n",
    "\n",
    "LLMs work by predicting the next token (word or subword) given a sequence of previous tokens. This simple idea is the foundation of how models like GPT, LLaMA, and others generate text.\n",
    "\n",
    "**Key Concept:** Given \"The cat sat on the\", the model predicts \"mat\" (or \"chair\", \"floor\", etc.) based on probabilities.\n",
    "\n",
    "Let's see this in action with GPT-2, a small open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 small model (124M parameters)\n",
    "print(\"Loading GPT-2 model...\")\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "print(f\"Model size: ~124M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Next Token Prediction\n",
    "\n",
    "Let's see what the model predicts as the next token for different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_tokens(text, top_k=10):\n",
    "    \"\"\"\n",
    "    Predict the most likely next tokens given input text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text prompt\n",
    "        top_k: Number of top predictions to show\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        predictions = outputs.logits\n",
    "    \n",
    "    # Get the predictions for the next token (last position)\n",
    "    next_token_logits = predictions[0, -1, :]\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # Get top k predictions\n",
    "    top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìù Input: '{text}'\\n\")\n",
    "    print(\"Top predictions for the next token:\\n\")\n",
    "    print(f\"{'Rank':<6} {'Token':<20} {'Probability':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for rank, (prob, idx) in enumerate(zip(top_probs, top_indices), 1):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"{rank:<6} {repr(token):<20} {prob.item():.4f} ({prob.item()*100:.2f}%)\")\n",
    "    \n",
    "    return top_probs, top_indices\n",
    "\n",
    "# Example 1: Simple completion\n",
    "predict_next_tokens(\"The capital of Rwanda is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Another completion\n",
    "predict_next_tokens(\"Once upon a time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Technical context\n",
    "predict_next_tokens(\"Machine learning is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 1: Experiment with Next Token Prediction\n",
    "\n",
    "Try different prompts and observe:\n",
    "1. How do probabilities change with different contexts?\n",
    "2. What happens with ambiguous prompts?\n",
    "3. Try prompts in different languages (if the model supports them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Try your own prompts here:\n",
    "your_prompt = \"The weather today is\"  # Change this!\n",
    "predict_next_tokens(your_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Probability Distribution\n",
    "\n",
    "Let's visualize how confident the model is about different predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(text, top_k=15):\n",
    "    \"\"\"\n",
    "    Visualize the probability distribution of next token predictions.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "    tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(range(top_k), top_probs.numpy())\n",
    "    plt.yticks(range(top_k), [f\"{i+1}. {repr(t)}\" for i, t in enumerate(tokens)])\n",
    "    plt.xlabel('Probability')\n",
    "    plt.title(f'Top {top_k} Next Token Predictions for: \"{text}\"')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "visualize_predictions(\"The capital of Rwanda is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Temperature in Text Generation\n",
    "\n",
    "Temperature controls the randomness of predictions. Let's see how it affects generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temperature(prompt, temperature=1.0, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate text with different temperature settings.\n",
    "    \n",
    "    Temperature:\n",
    "    - Low (0.1-0.5): More deterministic, focused\n",
    "    - Medium (0.7-1.0): Balanced\n",
    "    - High (1.5-2.0): More random, creative\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Temperature: {temperature}\")\n",
    "    print(f\"Generated: {generated_text}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "prompt = \"Artificial intelligence will\"\n",
    "\n",
    "print(\"Comparing different temperatures:\\n\")\n",
    "generate_with_temperature(prompt, temperature=0.3)\n",
    "generate_with_temperature(prompt, temperature=1.0)\n",
    "generate_with_temperature(prompt, temperature=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Tokenization - Breaking Text into Pieces\n",
    "\n",
    "## What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens) that the model can process. Different languages and writing systems require different tokenization strategies.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Tokens can be words, subwords, or characters\n",
    "- Different tokenizers handle different languages differently\n",
    "- Languages with rich morphology (like Kinyarwanda) may be tokenized less efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Different Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different tokenizers\n",
    "print(\"Loading different tokenizers...\\n\")\n",
    "\n",
    "tokenizers_to_compare = {\n",
    "    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"BERT\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    \"RoBERTa\": AutoTokenizer.from_pretrained(\"roberta-base\"),\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Tokenizers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenization(text, tokenizers_dict):\n",
    "    \"\"\"\n",
    "    Compare how different tokenizers process the same text.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìù Original text: '{text}'\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name, tokenizer in tokenizers_dict.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Number of tokens: {len(tokens)}\")\n",
    "        print(f\"  Tokens: {tokens}\")\n",
    "        print(f\"  Token IDs: {token_ids}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Example 1: English text\n",
    "compare_tokenization(\"Hello, how are you today?\", tokenizers_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Technical text\n",
    "compare_tokenization(\"Machine learning is revolutionizing technology.\", tokenizers_to_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization for Different Languages\n",
    "\n",
    "Let's see how tokenization works for different languages, including Kinyarwanda. This is important because most tokenizers are trained primarily on English data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences in different languages\n",
    "multilingual_examples = {\n",
    "    \"English\": \"Hello, how are you?\",\n",
    "    \"Kinyarwanda\": \"Mwaramutse, mumeze mute?\",\n",
    "    \"French\": \"Bonjour, comment allez-vous?\",\n",
    "    \"Swahili\": \"Habari, unajisikiaje?\",\n",
    "    \"Spanish\": \"Hola, ¬øc√≥mo est√°s?\",\n",
    "}\n",
    "\n",
    "def analyze_multilingual_tokenization(examples, tokenizer, tokenizer_name):\n",
    "    \"\"\"\n",
    "    Analyze how a tokenizer handles different languages.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Tokenizer: {tokenizer_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for language, text in examples.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        num_tokens = len(tokens)\n",
    "        num_chars = len(text)\n",
    "        efficiency = num_chars / num_tokens if num_tokens > 0 else 0\n",
    "        \n",
    "        results[language] = {\n",
    "            'tokens': tokens,\n",
    "            'num_tokens': num_tokens,\n",
    "            'num_chars': num_chars,\n",
    "            'efficiency': efficiency\n",
    "        }\n",
    "        \n",
    "        print(f\"{language}:\")\n",
    "        print(f\"  Text: '{text}'\")\n",
    "        print(f\"  Tokens: {tokens}\")\n",
    "        print(f\"  Number of tokens: {num_tokens}\")\n",
    "        print(f\"  Characters per token: {efficiency:.2f}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze with GPT-2 tokenizer\n",
    "gpt2_results = analyze_multilingual_tokenization(\n",
    "    multilingual_examples, \n",
    "    tokenizers_to_compare[\"GPT-2\"],\n",
    "    \"GPT-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tokenization efficiency across languages\n",
    "def visualize_tokenization_efficiency(results):\n",
    "    \"\"\"\n",
    "    Visualize how efficiently different languages are tokenized.\n",
    "    \"\"\"\n",
    "    languages = list(results.keys())\n",
    "    num_tokens = [results[lang]['num_tokens'] for lang in languages]\n",
    "    efficiency = [results[lang]['efficiency'] for lang in languages]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Number of tokens\n",
    "    ax1.bar(languages, num_tokens, color='steelblue')\n",
    "    ax1.set_ylabel('Number of Tokens')\n",
    "    ax1.set_title('Number of Tokens per Language')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Efficiency (chars per token)\n",
    "    ax2.bar(languages, efficiency, color='coral')\n",
    "    ax2.set_ylabel('Characters per Token')\n",
    "    ax2.set_title('Tokenization Efficiency (Higher = More Efficient)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_tokenization_efficiency(gpt2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Explore Tokenization\n",
    "\n",
    "### Part A: Experiment with Different Texts\n",
    "\n",
    "Try tokenizing:\n",
    "1. Long Kinyarwanda sentences\n",
    "2. Technical terms in Kinyarwanda\n",
    "3. Mixed language text (code-switching)\n",
    "\n",
    "**Questions to consider:**\n",
    "- Which languages are tokenized more efficiently?\n",
    "- Why might some languages require more tokens?\n",
    "- What are the implications for LLM performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Add your own examples\n",
    "your_examples = {\n",
    "    \"Example 1\": \"Add your text here\",\n",
    "    \"Example 2\": \"Add another example\",\n",
    "    # Add more examples\n",
    "}\n",
    "\n",
    "# Uncomment to test:\n",
    "# your_results = analyze_multilingual_tokenization(your_examples, tokenizers_to_compare[\"GPT-2\"], \"GPT-2\")\n",
    "# visualize_tokenization_efficiency(your_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: OpenAI Tokenizer Playground\n",
    "\n",
    "**üìé Online Exercise:**\n",
    "\n",
    "Visit the OpenAI Tokenizer Playground: https://platform.openai.com/tokenizer\n",
    "\n",
    "**Tasks:**\n",
    "1. Test the same Kinyarwanda sentences you used above\n",
    "2. Compare the token counts with GPT-2\n",
    "3. Try different GPT models (GPT-3.5, GPT-4) and observe differences\n",
    "4. Experiment with:\n",
    "   - Punctuation\n",
    "   - Numbers\n",
    "   - Special characters\n",
    "   - Emojis\n",
    "\n",
    "**Discussion Points:**\n",
    "- Why do newer models (GPT-4) tokenize some languages more efficiently?\n",
    "- What does this mean for cost and performance?\n",
    "- How might this affect model training on low-resource languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Subword Tokenization\n",
    "\n",
    "Let's visualize how subword tokenization works with a detailed example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_subword_tokens(text, tokenizer, tokenizer_name):\n",
    "    \"\"\"\n",
    "    Visualize how text is broken into subword tokens.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    print(f\"\\nTokenizer: {tokenizer_name}\")\n",
    "    print(f\"Original text: '{text}'\")\n",
    "    print(f\"\\nToken breakdown:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, token in enumerate(tokens, 1):\n",
    "        # Show the token and its representation\n",
    "        token_clean = token.replace('ƒ†', '‚ñÅ')  # Show spaces as ‚ñÅ\n",
    "        token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
    "        print(f\"Token {i:2d}: {token_clean:20s} (ID: {token_id})\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total tokens: {len(tokens)}\\n\")\n",
    "\n",
    "# Example with uncommon/technical words\n",
    "examples = [\n",
    "    \"The biotechnology industry is growing.\",\n",
    "    \"Umunyarwanda w'umwanditsi\",  # Kinyarwanda\n",
    "    \"Preprocessing and tokenization\",\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    visualize_subword_tokens(example, tokenizers_to_compare[\"GPT-2\"], \"GPT-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Vector Embeddings - Representing Meaning\n",
    "\n",
    "## What are Embeddings?\n",
    "\n",
    "Embeddings are numerical representations (vectors) of tokens that capture their meaning. Similar words have similar embeddings.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Each token is represented as a vector of numbers (typically 768 or 1024 dimensions)\n",
    "- Similar meanings ‚Üí Similar vectors\n",
    "- We can measure similarity using cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Embeddings from GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Get the embedding vector for a word.\n",
    "    \"\"\"\n",
    "    # Get token ID\n",
    "    token_id = tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "    \n",
    "    # Get embedding from model's embedding layer\n",
    "    embedding = model.transformer.wte.weight[token_id].detach().numpy()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Get embeddings for some words\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"cat\", \"dog\", \"computer\", \"phone\"]\n",
    "embeddings = {}\n",
    "\n",
    "for word in words:\n",
    "    embeddings[word] = get_word_embedding(word, model, tokenizer)\n",
    "    print(f\"‚úÖ Embedding for '{word}': shape {embeddings[word].shape}\")\n",
    "\n",
    "print(f\"\\nEmbedding dimension: {embeddings[words[0]].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Similarity Between Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(words, embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between all pairs of words.\n",
    "    \"\"\"\n",
    "    n = len(words)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            emb1 = embeddings[word1].reshape(1, -1)\n",
    "            emb2 = embeddings[word2].reshape(1, -1)\n",
    "            similarity_matrix[i, j] = cosine_similarity(emb1, emb2)[0, 0]\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def visualize_similarity_matrix(words, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Visualize the similarity matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, \n",
    "                xticklabels=words, \n",
    "                yticklabels=words,\n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                cmap='coolwarm',\n",
    "                center=0.5,\n",
    "                vmin=0,\n",
    "                vmax=1)\n",
    "    plt.title('Cosine Similarity Between Word Embeddings')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute and visualize similarities\n",
    "similarity_matrix = compute_similarity_matrix(words, embeddings)\n",
    "visualize_similarity_matrix(words, similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Similarity Scores\n",
    "\n",
    "**What do the numbers mean?**\n",
    "- 1.0: Identical (same word)\n",
    "- 0.8-0.9: Very similar meaning\n",
    "- 0.6-0.7: Related concepts\n",
    "- 0.4-0.5: Some relation\n",
    "- < 0.4: Not very related\n",
    "\n",
    "**Observations from the heatmap:**\n",
    "- Words with similar meanings have higher similarity scores\n",
    "- Semantic relationships are captured (e.g., king-queen, man-woman)\n",
    "- Category relationships (e.g., cat-dog, computer-phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(target_word, words, embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the most similar words to a target word.\n",
    "    \"\"\"\n",
    "    target_emb = embeddings[target_word].reshape(1, -1)\n",
    "    similarities = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word != target_word:\n",
    "            emb = embeddings[word].reshape(1, -1)\n",
    "            sim = cosine_similarity(target_emb, emb)[0, 0]\n",
    "            similarities.append((word, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nWords most similar to '{target_word}':\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (word, sim) in enumerate(similarities[:top_k], 1):\n",
    "        print(f\"{i}. {word:<15} (similarity: {sim:.4f})\")\n",
    "\n",
    "find_most_similar(\"king\", words, embeddings)\n",
    "find_most_similar(\"computer\", words, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings in 2D\n",
    "\n",
    "Embeddings exist in high-dimensional space (768 dimensions for GPT-2). We can use dimensionality reduction to visualize them in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings_2d(words, embeddings):\n",
    "    \"\"\"\n",
    "    Visualize embeddings in 2D using PCA.\n",
    "    \"\"\"\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.array([embeddings[word] for word in words])\n",
    "    \n",
    "    # Reduce to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embedding_matrix)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100, alpha=0.6)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, \n",
    "                    (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                    fontsize=12,\n",
    "                    ha='center',\n",
    "                    va='bottom')\n",
    "    \n",
    "    plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.title('Word Embeddings Visualized in 2D (PCA)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTotal variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "visualize_embeddings_2d(words, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Arithmetic: The Famous \"King - Man + Woman = Queen\" Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_arithmetic_example(embeddings, tokenizer):\n",
    "    \"\"\"\n",
    "    Demonstrate vector arithmetic with embeddings.\n",
    "    \"\"\"\n",
    "    # Get embeddings\n",
    "    king_emb = embeddings['king']\n",
    "    man_emb = embeddings['man']\n",
    "    woman_emb = embeddings['woman']\n",
    "    \n",
    "    # Compute: king - man + woman\n",
    "    result_emb = king_emb - man_emb + woman_emb\n",
    "    \n",
    "    # Find closest word to result\n",
    "    vocab_size = len(tokenizer)\n",
    "    all_embeddings = model.transformer.wte.weight.detach().numpy()\n",
    "    \n",
    "    # Compute similarities with all words (sample first 5000 for speed)\n",
    "    sample_size = min(5000, vocab_size)\n",
    "    similarities = cosine_similarity([result_emb], all_embeddings[:sample_size])[0]\n",
    "    \n",
    "    # Get top 10 matches\n",
    "    top_indices = np.argsort(similarities)[::-1][:10]\n",
    "    \n",
    "    print(\"Vector Arithmetic: king - man + woman = ?\\n\")\n",
    "    print(\"Top 10 closest words:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        word = tokenizer.decode([idx])\n",
    "        sim = similarities[idx]\n",
    "        print(f\"{i:2d}. {word:<20} (similarity: {sim:.4f})\")\n",
    "\n",
    "vector_arithmetic_example(embeddings, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring More Word Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore more semantic categories\n",
    "semantic_groups = {\n",
    "    \"Royalty\": [\"king\", \"queen\", \"prince\", \"princess\"],\n",
    "    \"Animals\": [\"cat\", \"dog\", \"lion\", \"tiger\"],\n",
    "    \"Technology\": [\"computer\", \"phone\", \"internet\", \"software\"],\n",
    "    \"Countries\": [\"France\", \"Rwanda\", \"Japan\", \"Brazil\"],\n",
    "}\n",
    "\n",
    "# Get embeddings for all words\n",
    "all_words = []\n",
    "all_embeddings = {}\n",
    "\n",
    "for category, words_list in semantic_groups.items():\n",
    "    for word in words_list:\n",
    "        try:\n",
    "            all_embeddings[word] = get_word_embedding(word, model, tokenizer)\n",
    "            all_words.append(word)\n",
    "        except:\n",
    "            print(f\"Could not get embedding for: {word}\")\n",
    "\n",
    "print(f\"\\nGot embeddings for {len(all_words)} words\")\n",
    "\n",
    "# Visualize all semantic groups\n",
    "if len(all_words) > 0:\n",
    "    visualize_embeddings_2d(all_words, all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 3: Explore Embeddings\n",
    "\n",
    "### Part A: Custom Word Lists\n",
    "\n",
    "Create your own word lists and explore their embeddings:\n",
    "\n",
    "**Suggested explorations:**\n",
    "1. Professional titles (doctor, teacher, engineer, farmer)\n",
    "2. Colors (red, blue, green, yellow)\n",
    "3. Emotions (happy, sad, angry, excited)\n",
    "4. Foods (rice, bread, banana, coffee)\n",
    "5. Kinyarwanda words (if available in tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Create your own word list\n",
    "your_words = [\n",
    "    \"doctor\", \"teacher\", \"engineer\", \"farmer\",\n",
    "    \"hospital\", \"school\", \"office\", \"farm\"\n",
    "]\n",
    "\n",
    "# Get embeddings\n",
    "your_embeddings = {}\n",
    "for word in your_words:\n",
    "    try:\n",
    "        your_embeddings[word] = get_word_embedding(word, model, tokenizer)\n",
    "    except:\n",
    "        print(f\"Skipping: {word}\")\n",
    "\n",
    "# Analyze\n",
    "if len(your_embeddings) > 1:\n",
    "    print(\"\\nSimilarity Analysis:\")\n",
    "    valid_words = list(your_embeddings.keys())\n",
    "    sim_matrix = compute_similarity_matrix(valid_words, your_embeddings)\n",
    "    visualize_similarity_matrix(valid_words, sim_matrix)\n",
    "    visualize_embeddings_2d(valid_words, your_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Vector Arithmetic Experiments\n",
    "\n",
    "Try your own vector arithmetic:\n",
    "- Paris - France + Rwanda = ?\n",
    "- Doctor - Hospital + School = ?\n",
    "- Computer - Technology + Nature = ?\n",
    "\n",
    "Think about:\n",
    "- What relationships are captured?\n",
    "- What relationships are missed?\n",
    "- Why might some analogies work better than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom vector arithmetic here\n",
    "# Example: word1 - word2 + word3 = ?\n",
    "\n",
    "def custom_vector_arithmetic(word1, word2, word3, tokenizer, model, top_k=10):\n",
    "    \"\"\"\n",
    "    Compute: word1 - word2 + word3 = ?\n",
    "    \"\"\"\n",
    "    try:\n",
    "        emb1 = get_word_embedding(word1, model, tokenizer)\n",
    "        emb2 = get_word_embedding(word2, model, tokenizer)\n",
    "        emb3 = get_word_embedding(word3, model, tokenizer)\n",
    "        \n",
    "        result = emb1 - emb2 + emb3\n",
    "        \n",
    "        # Find closest words\n",
    "        all_embeddings = model.transformer.wte.weight.detach().numpy()\n",
    "        similarities = cosine_similarity([result], all_embeddings[:5000])[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        print(f\"\\n{word1} - {word2} + {word3} = ?\\n\")\n",
    "        print(\"Top matches:\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, idx in enumerate(top_indices, 1):\n",
    "            word = tokenizer.decode([idx])\n",
    "            print(f\"{i:2d}. {word:<20} ({similarities[idx]:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Try some analogies\n",
    "custom_vector_arithmetic(\"Paris\", \"France\", \"Rwanda\", tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Vector Databases - Storing and Retrieving Documents\n",
    "\n",
    "## What are Vector Databases?\n",
    "\n",
    "Vector databases are specialized databases designed to store and efficiently search through vector embeddings. They are crucial for:\n",
    "- **Semantic search**: Finding documents by meaning, not just keywords\n",
    "- **RAG (Retrieval Augmented Generation)**: Providing LLMs with relevant context\n",
    "- **Recommendation systems**: Finding similar items\n",
    "- **Question answering**: Retrieving relevant information\n",
    "\n",
    "**How it works:**\n",
    "1. Convert documents into embeddings\n",
    "2. Store embeddings in a vector database\n",
    "3. Convert user queries into embeddings\n",
    "4. Find similar documents using vector similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Vector Database Libraries\n",
    "\n",
    "We'll use **ChromaDB** - a lightweight, open-source vector database perfect for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ChromaDB and sentence-transformers for better embeddings\n",
    "!pip install chromadb sentence-transformers --quiet\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "import uuid\n",
    "\n",
    "print(\"‚úÖ Vector database libraries installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Sample Document Collection\n",
    "\n",
    "Let's create a collection of documents about Rwanda that we'll store in our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about Rwanda and technology\n",
    "documents = [\n",
    "    {\n",
    "        \"text\": \"Rwanda is a landlocked country in East Africa, known as the land of a thousand hills. The capital city is Kigali.\",\n",
    "        \"metadata\": {\"category\": \"geography\", \"topic\": \"rwanda_overview\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Kigali is one of the cleanest cities in Africa. It has modern infrastructure and is a growing technology hub.\",\n",
    "        \"metadata\": {\"category\": \"cities\", \"topic\": \"kigali\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Rwanda has made significant progress in technology adoption. The country has invested heavily in ICT infrastructure and digital literacy.\",\n",
    "        \"metadata\": {\"category\": \"technology\", \"topic\": \"digital_transformation\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Machine learning and artificial intelligence are emerging fields in Rwanda. Several startups are working on AI solutions for agriculture and healthcare.\",\n",
    "        \"metadata\": {\"category\": \"technology\", \"topic\": \"ai_ml\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Kinyarwanda is the national language of Rwanda, spoken by most of the population. French and English are also official languages.\",\n",
    "        \"metadata\": {\"category\": \"language\", \"topic\": \"kinyarwanda\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The African Institute for Mathematical Sciences (AIMS) in Rwanda provides advanced training in mathematical sciences and data science.\",\n",
    "        \"metadata\": {\"category\": \"education\", \"topic\": \"aims\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Rwanda's economy has grown rapidly, with technology and services sectors leading the growth. The country aims to become a knowledge-based economy.\",\n",
    "        \"metadata\": {\"category\": \"economy\", \"topic\": \"growth\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Natural language processing for Kinyarwanda is an active research area. Challenges include limited training data and unique linguistic features.\",\n",
    "        \"metadata\": {\"category\": \"technology\", \"topic\": \"nlp_kinyarwanda\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Mountain gorillas can be found in the Volcanoes National Park in Rwanda. Gorilla trekking is a major tourist attraction.\",\n",
    "        \"metadata\": {\"category\": \"tourism\", \"topic\": \"wildlife\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Rwanda has implemented various digital government services. Citizens can access many government services online through the Irembo platform.\",\n",
    "        \"metadata\": {\"category\": \"technology\", \"topic\": \"e_government\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìö Created {len(documents)} sample documents\")\n",
    "print(\"\\nSample document:\")\n",
    "print(f\"Text: {documents[0]['text']}\")\n",
    "print(f\"Metadata: {documents[0]['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Embedding Model\n",
    "\n",
    "We'll use a sentence transformer model that's optimized for creating semantic embeddings of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentence transformer model\n",
    "# Using 'all-MiniLM-L6-v2' - a good balance of quality and speed\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"‚úÖ Model loaded: all-MiniLM-L6-v2\")\n",
    "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Test the model\n",
    "test_text = \"Rwanda is a beautiful country\"\n",
    "test_embedding = embedding_model.encode(test_text)\n",
    "print(f\"\\nTest embedding shape: {test_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vector Database\n",
    "\n",
    "Now let's create a ChromaDB database and add our documents to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    allow_reset=True\n",
    "))\n",
    "\n",
    "# Create or get a collection\n",
    "collection_name = \"rwanda_documents\"\n",
    "collection = chroma_client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\": \"Collection of documents about Rwanda\"}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Documents to the Vector Database\n",
    "\n",
    "Let's convert our documents to embeddings and store them in ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_collection(documents, collection, embedding_model):\n",
    "    \"\"\"\n",
    "    Convert documents to embeddings and add them to the collection.\n",
    "    \"\"\"\n",
    "    print(\"Converting documents to embeddings...\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        # Create embedding\n",
    "        embedding = embedding_model.encode(doc[\"text\"]).tolist()\n",
    "        \n",
    "        # Generate unique ID\n",
    "        doc_id = f\"doc_{i}\"\n",
    "        \n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            embeddings=[embedding],\n",
    "            documents=[doc[\"text\"]],\n",
    "            metadatas=[doc[\"metadata\"]],\n",
    "            ids=[doc_id]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Added document {i}/{len(documents)}: {doc['text'][:60]}...\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully added {len(documents)} documents to the vector database!\")\n",
    "\n",
    "# Add all documents to the collection\n",
    "add_documents_to_collection(documents, collection, embedding_model)\n",
    "\n",
    "# Verify the count\n",
    "print(f\"\\nTotal documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search: Querying the Vector Database\n",
    "\n",
    "Now comes the exciting part - searching for relevant documents based on the meaning of our query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, collection, embedding_model, n_results=3):\n",
    "    \"\"\"\n",
    "    Perform semantic search on the vector database.\n",
    "    \n",
    "    Args:\n",
    "        query: Text query to search for\n",
    "        collection: ChromaDB collection\n",
    "        embedding_model: Model to create query embedding\n",
    "        n_results: Number of results to return\n",
    "    \"\"\"\n",
    "    # Convert query to embedding\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    \n",
    "    # Search the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüîç Query: '{query}'\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTop {n_results} most relevant documents:\\n\")\n",
    "    \n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ), 1):\n",
    "        similarity = 1 - distance  # Convert distance to similarity\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  Similarity: {similarity:.4f} ({similarity*100:.2f}%)\")\n",
    "        print(f\"  Category: {metadata['category']}\")\n",
    "        print(f\"  Text: {doc}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example queries\n",
    "queries = [\n",
    "    \"Tell me about artificial intelligence in Rwanda\",\n",
    "    \"What is the capital city?\",\n",
    "    \"Information about Kinyarwanda language\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    semantic_search(query, collection, embedding_model)\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Semantic Matching**: The search finds relevant documents even when they don't contain the exact query words\n",
    "2. **Similarity Scores**: Higher scores (closer to 1.0) indicate more relevant documents\n",
    "3. **Context Awareness**: The system understands that \"artificial intelligence\" relates to \"machine learning\" and \"technology\"\n",
    "\n",
    "**Compare this to keyword search:**\n",
    "- Keyword search: Looks for exact word matches\n",
    "- Semantic search: Understands meaning and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Query Results\n",
    "\n",
    "Let's visualize how queries relate to documents in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_query_results(query, documents, embedding_model, n_results=5):\n",
    "    \"\"\"\n",
    "    Visualize query and document embeddings in 2D space.\n",
    "    \"\"\"\n",
    "    # Get embeddings for all documents and query\n",
    "    doc_texts = [doc['text'] for doc in documents]\n",
    "    doc_embeddings = embedding_model.encode(doc_texts)\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    \n",
    "    # Combine all embeddings\n",
    "    all_embeddings = np.vstack([doc_embeddings, query_embedding.reshape(1, -1)])\n",
    "    \n",
    "    # Reduce to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(all_embeddings)\n",
    "    \n",
    "    # Split back into docs and query\n",
    "    doc_embeddings_2d = embeddings_2d[:-1]\n",
    "    query_embedding_2d = embeddings_2d[-1]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][:n_results]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Plot documents\n",
    "    colors = plt.cm.viridis(similarities)\n",
    "    scatter = plt.scatter(doc_embeddings_2d[:, 0], doc_embeddings_2d[:, 1], \n",
    "                         c=similarities, cmap='viridis', s=100, alpha=0.6, \n",
    "                         edgecolors='black', linewidth=1)\n",
    "    \n",
    "    # Plot query\n",
    "    plt.scatter(query_embedding_2d[0], query_embedding_2d[1], \n",
    "               c='red', s=300, marker='*', edgecolors='black', \n",
    "               linewidth=2, label='Query', zorder=5)\n",
    "    \n",
    "    # Add labels for top results\n",
    "    for idx in top_indices:\n",
    "        plt.annotate(f\"Doc {idx+1}\\n({similarities[idx]:.3f})\",\n",
    "                    (doc_embeddings_2d[idx, 0], doc_embeddings_2d[idx, 1]),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
    "                    fontsize=9, ha='left')\n",
    "    \n",
    "    # Add query label\n",
    "    plt.annotate('Query',\n",
    "                (query_embedding_2d[0], query_embedding_2d[1]),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='red', alpha=0.7),\n",
    "                fontsize=10, fontweight='bold', ha='left')\n",
    "    \n",
    "    plt.colorbar(scatter, label='Similarity to Query')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title(f'Document Embeddings vs Query: \"{query}\"\\n(Brighter colors = Higher similarity)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a query\n",
    "visualize_query_results(\n",
    "    \"What technology initiatives exist in Rwanda?\", \n",
    "    documents, \n",
    "    embedding_model,\n",
    "    n_results=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering with Metadata\n",
    "\n",
    "Vector databases allow you to combine semantic search with metadata filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_filter(query, collection, embedding_model, filter_dict, n_results=3):\n",
    "    \"\"\"\n",
    "    Perform semantic search with metadata filtering.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    \n",
    "    # Search with filter\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        where=filter_dict\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüîç Query: '{query}'\")\n",
    "    print(f\"üìã Filter: {filter_dict}\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not results['documents'][0]:\n",
    "        print(\"No documents found matching the filter.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nTop {len(results['documents'][0])} results:\\n\")\n",
    "    \n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ), 1):\n",
    "        similarity = 1 - distance\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  Similarity: {similarity:.4f}\")\n",
    "        print(f\"  Category: {metadata['category']}\")\n",
    "        print(f\"  Topic: {metadata['topic']}\")\n",
    "        print(f\"  Text: {doc}\")\n",
    "        print()\n",
    "\n",
    "# Example: Search only in technology category\n",
    "search_with_filter(\n",
    "    \"machine learning and AI\",\n",
    "    collection,\n",
    "    embedding_model,\n",
    "    filter_dict={\"category\": \"technology\"},\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Example: Search only in geography category\n",
    "search_with_filter(\n",
    "    \"beautiful landscapes\",\n",
    "    collection,\n",
    "    embedding_model,\n",
    "    filter_dict={\"category\": \"geography\"},\n",
    "    n_results=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Application: Building a Simple RAG System\n",
    "\n",
    "Let's combine our vector database with an LLM to build a basic Retrieval Augmented Generation (RAG) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_query(question, collection, embedding_model, llm_model, llm_tokenizer, n_context=2):\n",
    "    \"\"\"\n",
    "    Simple RAG: Retrieve relevant context and generate answer.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert question to embedding\n",
    "    2. Retrieve relevant documents\n",
    "    3. Use documents as context for LLM\n",
    "    4. Generate answer\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Step 1: Retrieve relevant documents\n",
    "    query_embedding = embedding_model.encode(question).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_context\n",
    "    )\n",
    "    \n",
    "    print(f\"üìö Retrieved {len(results['documents'][0])} relevant documents:\\n\")\n",
    "    context_docs = results['documents'][0]\n",
    "    for i, doc in enumerate(context_docs, 1):\n",
    "        print(f\"  {i}. {doc[:80]}...\")\n",
    "    \n",
    "    # Step 2: Build context\n",
    "    context = \"\\n\\n\".join(context_docs)\n",
    "    \n",
    "    # Step 3: Create prompt\n",
    "    prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    print(f\"\\nü§ñ Generating answer...\\n\")\n",
    "    \n",
    "    # Step 4: Generate answer\n",
    "    input_ids = llm_tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = llm_model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + 100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=llm_tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Extract only the generated answer (not the prompt)\n",
    "    answer = llm_tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"üí° Answer: {answer.strip()}\")\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    return answer, context_docs\n",
    "\n",
    "# Test RAG system\n",
    "questions = [\n",
    "    \"What technological developments are happening in Rwanda?\",\n",
    "    \"Tell me about education in Rwanda\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    simple_rag_query(question, collection, embedding_model, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Search Methods\n",
    "\n",
    "Let's compare semantic search with traditional keyword search to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(query, documents, n_results=3):\n",
    "    \"\"\"\n",
    "    Simple keyword-based search for comparison.\n",
    "    \"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    scores = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_words = set(doc['text'].lower().split())\n",
    "        # Count matching words\n",
    "        matches = len(query_words.intersection(doc_words))\n",
    "        scores.append(matches)\n",
    "    \n",
    "    # Get top results\n",
    "    top_indices = np.argsort(scores)[::-1][:n_results]\n",
    "    \n",
    "    print(f\"\\nüîç Keyword Search: '{query}'\\n\")\n",
    "    print(\"Results:\\n\")\n",
    "    \n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  Matching words: {scores[idx]}\")\n",
    "        print(f\"  Text: {documents[idx]['text']}\")\n",
    "        print()\n",
    "\n",
    "def compare_search_methods(query, documents, collection, embedding_model):\n",
    "    \"\"\"\n",
    "    Compare semantic search vs keyword search.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Comparing Search Methods for: '{query}'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Keyword search\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"METHOD 1: KEYWORD SEARCH (Traditional)\")\n",
    "    print(\"-\"*80)\n",
    "    keyword_search(query, documents, n_results=3)\n",
    "    \n",
    "    # Semantic search\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"METHOD 2: SEMANTIC SEARCH (Vector Database)\")\n",
    "    print(\"-\"*80)\n",
    "    semantic_search(query, collection, embedding_model, n_results=3)\n",
    "\n",
    "# Test with queries that demonstrate the difference\n",
    "test_queries = [\n",
    "    \"AI and machine learning innovations\",  # Uses different words than documents\n",
    "    \"cleanest urban areas in Africa\",  # Synonymous concept\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    compare_search_methods(query, documents, collection, embedding_model)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 4: Build Your Own Vector Database\n",
    "\n",
    "### Part A: Create Your Own Document Collection\n",
    "\n",
    "**Task:** Create a collection of documents about a topic of your choice:\n",
    "1. Pick a topic (e.g., Rwandan history, technology startups, agriculture, education)\n",
    "2. Create 8-10 documents about this topic\n",
    "3. Add relevant metadata to each document\n",
    "4. Store them in a vector database\n",
    "\n",
    "**Bonus:** Include some documents in Kinyarwanda if the embedding model supports it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Create your own document collection\n",
    "my_documents = [\n",
    "    {\n",
    "        \"text\": \"Your document text here\",\n",
    "        \"metadata\": {\"category\": \"your_category\", \"topic\": \"your_topic\"}\n",
    "    },\n",
    "    # Add more documents...\n",
    "]\n",
    "\n",
    "# Create a new collection\n",
    "# my_collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "# add_documents_to_collection(my_documents, my_collection, embedding_model)\n",
    "\n",
    "# Test with queries\n",
    "# semantic_search(\"your query\", my_collection, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Experiment with Different Queries\n",
    "\n",
    "**Tasks:**\n",
    "1. Try synonymous queries (e.g., \"AI\" vs \"artificial intelligence\")\n",
    "2. Try queries in different languages\n",
    "3. Experiment with metadata filtering\n",
    "4. Compare semantic vs keyword search results\n",
    "\n",
    "**Questions to consider:**\n",
    "- How does semantic search handle synonyms?\n",
    "- What happens with very short vs very long queries?\n",
    "- How does the number of documents affect search quality?\n",
    "- How could you improve retrieval accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment space\n",
    "# Try different queries and observe the results\n",
    "\n",
    "my_query = \"Your experimental query here\"\n",
    "# semantic_search(my_query, collection, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts Summary: Vector Databases\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Vector Databases** store embeddings for efficient similarity search\n",
    "2. **Semantic Search** finds documents by meaning, not just keywords\n",
    "3. **RAG Systems** combine retrieval with generation for better answers\n",
    "4. **Metadata Filtering** allows hybrid search (semantic + traditional filters)\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "1. **Question Answering**: Find relevant information to answer user queries\n",
    "2. **Document Search**: Search large document collections semantically\n",
    "3. **Recommendation Systems**: Find similar items/content\n",
    "4. **Chatbots**: Provide contextual responses using relevant documents\n",
    "5. **Knowledge Management**: Organize and retrieve organizational knowledge\n",
    "\n",
    "### Popular Vector Databases\n",
    "\n",
    "- **ChromaDB**: Lightweight, great for prototyping (used here)\n",
    "- **Pinecone**: Managed cloud service\n",
    "- **Weaviate**: Open-source with GraphQL\n",
    "- **Milvus**: High-performance, scalable\n",
    "- **FAISS**: Facebook's similarity search library\n",
    "- **Qdrant**: Written in Rust, high performance\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Choose the right embedding model**: Balance between quality and speed\n",
    "2. **Chunk documents appropriately**: Not too large, not too small\n",
    "3. **Add good metadata**: Enables filtering and better organization\n",
    "4. **Test with real queries**: Evaluate retrieval quality\n",
    "5. **Monitor performance**: Track search latency and accuracy\n",
    "\n",
    "---\n",
    "# Summary and Key Takeaways\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "### 1. Next Token Prediction\n",
    "- LLMs predict the next token based on probability distributions\n",
    "- Temperature controls randomness in generation\n",
    "- The model assigns probabilities to thousands of possible next tokens\n",
    "\n",
    "### 2. Tokenization\n",
    "- Text is broken into tokens (words, subwords, or characters)\n",
    "- Different tokenizers handle languages differently\n",
    "- Languages with less training data are often tokenized less efficiently\n",
    "- Kinyarwanda and other low-resource languages may require more tokens\n",
    "- This affects both cost (API pricing) and model performance\n",
    "\n",
    "### 3. Vector Embeddings\n",
    "- Words are represented as vectors in high-dimensional space\n",
    "- Similar meanings have similar vectors\n",
    "- We can measure similarity using cosine similarity\n",
    "- Embeddings capture semantic relationships\n",
    "- Vector arithmetic can reveal word analogies\n",
    "\n",
    "### 4. Vector Databases\n",
    "- Store and efficiently search through embeddings\n",
    "- Enable semantic search based on meaning, not just keywords\n",
    "- Essential for RAG (Retrieval Augmented Generation) systems\n",
    "- Support metadata filtering for hybrid search\n",
    "- Power many real-world AI applications\n",
    "\n",
    "## Important Implications\n",
    "\n",
    "### For Low-Resource Languages (like Kinyarwanda):\n",
    "1. **Tokenization Challenges**: More tokens needed ‚Üí Higher costs, longer context\n",
    "2. **Representation**: Fewer examples in training data ‚Üí Potentially less accurate\n",
    "3. **Solutions**:\n",
    "   - Train language-specific tokenizers\n",
    "   - Use multilingual models\n",
    "   - Fine-tune on local language data\n",
    "   - Develop community datasets\n",
    "\n",
    "### For Model Development:\n",
    "1. Tokenization strategy affects model performance\n",
    "2. Embeddings quality depends on training data\n",
    "3. Context length limitations impact what the model can process\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Explore More Models**: Try different open-source models (Llama, Mistral, etc.)\n",
    "2. **Build Custom Tokenizers**: Create tokenizers optimized for Kinyarwanda\n",
    "3. **Fine-tuning**: Adapt models for specific tasks or languages\n",
    "4. **Contribute**: Help build datasets for low-resource languages\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- OpenAI Tokenizer: https://platform.openai.com/tokenizer\n",
    "- Hugging Face Transformers: https://huggingface.co/transformers/\n",
    "- Papers:\n",
    "  - \"Attention Is All You Need\" (Transformer architecture)\n",
    "  - \"Language Models are Few-Shot Learners\" (GPT-3)\n",
    "  - \"Neural Machine Translation of Rare Words with Subword Units\" (BPE)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Final Exercise: Reflection Questions\n",
    "\n",
    "1. How might tokenization inefficiency affect the cost of using LLMs for Kinyarwanda applications?\n",
    "2. What are some strategies to improve LLM performance for low-resource languages?\n",
    "3. How do embeddings capture meaning, and what are their limitations?\n",
    "4. Why is understanding these fundamentals important for building AI applications?\n",
    "5. How does semantic search in vector databases differ from traditional keyword search?\n",
    "6. What are the advantages of using RAG systems over standalone LLMs?\n",
    "7. How could vector databases be used to build applications for Rwanda?\n",
    "\n",
    "**Discussion**: Share your insights with your peers and instructor!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
