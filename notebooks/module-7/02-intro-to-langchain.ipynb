{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740ffa74-4eda-4843-9b5b-486caab1153b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to LangChain\n",
    "---------\n",
    "**AIMS-DSCB, 2025**\n",
    "\n",
    "**Author:** <font color='blue'>Dunstan Matekenya</font> \n",
    "\n",
    "**Affiliation:** <font color='#F88379'>DECAT, The World Bank Group</font> \n",
    "\n",
    "**Date:** <span style=\"color:#3EACAD\">November 5, 2025</span>\n",
    "\n",
    "\n",
    "## What you will learn \n",
    "In this notebook, you will learn the basics of the LangChain platform as follows.\n",
    "1. **LLM capabilities.** Explore LLM capabilities using LangChain\n",
    "2. **Interacting with LLMs.** Use LangChain functions such as chains, prompt templates and more to connect to LLMs\n",
    "3. **RAG.**. Implementing a simple RAG in Langchain by connecting to external documents\n",
    "4. **LangChain Expression Language (LCEL).**. How to use LCEL instead of functions when interacting with LLMs\n",
    "5. **LangChain Agents.**. \n",
    "\n",
    "## Expected Broad Learning Outcomes\n",
    "1. **Connecting to LLMs.** An understanding of how to connect to varios open source and proprietary LLMs using Hugging Face and proprietary specific frameworks such as that for OpenAI and Mistral\n",
    "2. **Different LLMs.**. There are many varieties of LLMs: ```chat, instruct, question-answer, sentiment-analysis, instruct``` and more. Have basic understanding of differences across these models and when to use which one.\n",
    "3. **The role of memory in Chat models.** Understand the importance of having memory in a chatbot and different strategies for doing it with LangChain.\n",
    "4. **The process of implementing RAG in LangChain**. RAG is one of the most commonly used approach for implementing chats as it enables connection to external custom data. Have a good understanding of the main steps involved in implementing a RAG based system-the steps are the same in LangChain and other frameworks.\n",
    "5. **Understand the role vector databases.** Vector databases are an integral part of working with LLMs. make sure you understand how they fit in the ecosystem and why they are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b7017-2001-4cec-b34d-30a5bc4b92fc",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a131a8d-40d2-4bf3-9856-103ed70000d7",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "We will import packages as we go so that you appreciate which class we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a13ee9-f3d9-4141-a1a2-929cdc1b5113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1dad1b-4014-48e9-b911-2095c9864a84",
   "metadata": {},
   "source": [
    "## Setup API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b443a7a8-9dd7-4320-958e-cc3376e09cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# Setup API Keys\n",
    "# ====================\n",
    "# Although its not recommended for security, you can also just \n",
    "# paste your API keys \n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61224418-685b-499c-96fe-568ba7993475",
   "metadata": {},
   "source": [
    "## Setup input directories \n",
    "Lets organize where our data is stored so that we can easily access it. Please refer to the slides for recommended folder setup. Copy and paste the full paths to your working folder in the variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a234a85f-b21b-4378-b3bd-f27feec67c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "DIR_DATA = Path.cwd().parents[1].joinpath(\"data\")\n",
    "\n",
    "DIR_DOCS = DIR_DATA.joinpath(\"docs-and-texts\")\n",
    "\n",
    "# Visit Rwanda PDF\n",
    "FILE_VISIT_RW = DIR_DOCS.joinpath(\"Visit-Rwanda-Destination-Guide.pdf\")\n",
    "\n",
    "FILE_DENGUE_GLOBAL = DIR_DOCS.joinpath(\"Dengue-Global-situation.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f148739-53c8-4cb8-b477-da5781e8195d",
   "metadata": {},
   "source": [
    "# 1. Exploring Language Tasks that LLMs can Perform\n",
    "In this section, we will explore what type of NLP tasks LLMs can perfom using the Hugging Face transformer package. In some cases, when we specifiy a specific model, the transformers package will take some time to download the model files. Also, the idea here is to show very simple capabilities. In a real world project, you can train and fine-tune the transformer models on your own dataset. For example, to do a fully fledged sentiment analysis with Hugging Face, take a look at [this tutorial] (https://huggingface.co/blog/sentiment-analysis-python).\n",
    "\n",
    ">Note that for almost all of these tasks, you can replace the English text with French text and still get similar results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc053ef-5ba8-4557-b0f2-0975447d566e",
   "metadata": {},
   "source": [
    "## 1. 1 Text and Document Classification\n",
    "Text and document classification are closely related tasks. In **text classification**, we assign predefined categories to individual pieces of text while in **document classification** refers to the process of assigning predefined categories to longer pieces of text, such as entire documents, articles, or reports.\n",
    "\n",
    "- **Examples of text classification tasks**. Sentiment Analysis; Intent Detection;\n",
    "- **Examples of document classification**. Topic categorization, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860fcb52-c3f4-4c8f-8fb4-e2b4808114c2",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with the Hugging Face Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499b7d29-d647-4cb9-ae00-fd2b88ae18b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmatekenya/My Drive (dmatekenya@gmail.com)/TEACHING/AIMS-DSCBI/.venv-llms/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# We use transformers ```pipeline library\n",
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-classification\", )\n",
    "\n",
    "text = \"I'm really enjoying my stay in Kigali, Rwanda!\"\n",
    "text2 = \" I really miss home and my family.\"\n",
    "outputs = llm(text)\n",
    "outputs2 = llm(text2)\n",
    "print(outputs[0]['label'])\n",
    "print(outputs2[0]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affcc68c-49ea-4cfe-bc59-294854156360",
   "metadata": {},
   "source": [
    "## 1.2 Text Generation\n",
    "Text generation is a process in natural language processing (NLP) where a machine learning model generates coherent and contextually relevant text based on a given input or prompt. This technology is used in various applications such as chatbots, automated content creation, machine translation, and more.\n",
    "\n",
    "In real life, the text is not always coherent, based on the model, when we use a default model, the results are not good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0224c530-1943-481e-b4fc-92cfb2f62702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malawi is famous for  a chai (the \"golden cup\") made of coconut milk, which is used in most Chinese restaurants. This was a popular method of cooking in the late 19th century as well as in Chinese restaurants in the Qing Dynasty.  The paste was called \"chai\" in Chinese, but the name Chai means \"golden cup\" and is often used for Chinese cooking.  The name \"chai\" is pronounced differently in Chinese and in Chinese restaurants, especially in Chinese restaurants.  It was known as chai (dong) in Chinese and \"chai\" in Chinese.  It is often translated by people as \"chai\" in Chinese.  In some Chinese restaurants, chai is used to add flavor to a dish.  For instance, when you add a hot sauce to a pan of rice, it is called chai dong.  A hot sauce is also called chai dong.  In this recipe, the batter of the hot sauce is called chai and the batter is called chai nakkul.  Chai dong is commonly used to add a little more flavor to a dish.  In the Chinese language, chai dong\n"
     ]
    }
   ],
   "source": [
    "llm = pipeline(\"text-generation\")\n",
    "prompt = \"Malawi is famous for \"\n",
    "outputs = llm(prompt, max_length=100)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabac32c-0547-44d4-b77b-34d16a2d8220",
   "metadata": {},
   "source": [
    "**EXERCISE-0: Try to specify a different Hugging Face model and see if you get better results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf9c35-35f0-4310-adff-6dbb7eab57f4",
   "metadata": {},
   "source": [
    "## 1.3 Text Summarization\n",
    "Text summarization is a natural language processing (NLP) task that involves creating a concise and coherent summary of a longer text document. The goal is to capture the most important information and main ideas while reducing the length of the original text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4c46a-b896-47ce-928b-0c8b8bae063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "long_text = \"\"\"Walking amid Gion's Machiya wooden houses is a mesmerizing experience. The beautifully\n",
    "preserved structures exuded an old-world charm that transports visitors back in time, making them feel\n",
    "like they had stepped into a living museum. The glow of lanterns lining the narrow streets add to the\n",
    "enchanting ambiance, making each stroll a memorable journey through Japan's rich cultural history.\n",
    "\"\"\"\n",
    "outputs = llm(long_text, max_length=60, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c10bc4-2237-45b2-a367-c7690a2586b4",
   "metadata": {},
   "source": [
    "## 1.4 Question-Answering\n",
    "Question Answering (QA) is one of the most common tasks or use casef for LLMs. In this task, the model is designed to automatically answer questions posed by humans in natural language. QA systems can be built to answer questions from a variety of sources, such as structured databases, knowledge bases, or unstructured text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aefbf4-a88a-4cdc-b882-eb0a5781200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = pipeline(\"question-answering\")\n",
    "context = \"Walking amid Gion's Machiya wooden houses was a mesmerizing experience.\"\n",
    "question = \"What are Machiya houses made of?\"\n",
    "outputs = llm(question=question, context=context)\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8c421-6a5d-426c-8f9f-fc94bc61b492",
   "metadata": {},
   "source": [
    "## 1.5 Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7750a-4ab4-4a5d-9019-7dc7be065719",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = pipeline(\"translation_en_to_fr\")\n",
    "text = \"This is my first time to visit Tunisia.\"\n",
    "outputs = llm(text, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3d82d-f4ea-439b-9fd5-08ae2105f3a3",
   "metadata": {},
   "source": [
    "# 2.  Introducing LangChain Core Functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c7bd8-c8cb-4e99-bef5-40144a82c78c",
   "metadata": {},
   "source": [
    "It is always a good idea to read documentation of a framework. Please head over to [LangChain website](https://www.langchain.com) for details of core functionalities, use cases and features. The screenshot below provides a summary of LangChain ecosytem of features and capabilities. The term **Chain** in LangChain refers to the core concept of **chains** in LangChain which is a sequence(s) of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL (we will see this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bc8a4-ea90-4622-92d8-43861fcb12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39d82c-7089-4aa4-8054-d01427f1c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/LangChain-detailed.png', width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dcce20-057f-4b5b-84b6-356a3c1db8a7",
   "metadata": {},
   "source": [
    "## 2.1 Interacting with Models in LangChain \n",
    "- General instruction models -  Models which can answer questions but are not quite optmized for chat\n",
    "- Chat models are more optimized for question and answering\n",
    "- Prompting templates and techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7bf7e-0ea4-431f-9f3e-23119e1a14a7",
   "metadata": {},
   "source": [
    "### Trying out Open Vs. Proprietary Model\n",
    "- **Accessing open source LLMs on Hugging Face.** In order to access open source LLMs from Hugging Face, you need two main inputs: ```Hugging Face token``` and the model id or url. Recall that you can explore and grab model details from the Hugging Face platform easily. Once you have that we can use ```HuggingFaceEndpoint``` or ```HuggingFaceHub``` to access and use the model.\n",
    "\n",
    "- **Accessing proprietary LLMs (e.g., OpenAI).** LangChain has specific packages for working with OpenAI models. For other providers such as Mistral, you need to check [LangChain documentation](https://python.langchain.com/v0.1/docs/integrations/chat/mistralai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0bf7a",
   "metadata": {},
   "source": [
    "## Try out OpenAI Models.\n",
    "In the same way, we can try several other models. \n",
    "However, we need to set up API keys first for almost all models providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35f1964-5938-4773-b768-334df1551939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Kigali is the capital and largest city of Rwanda, located in the central part of the country in the African Great Lakes region. It is situated in the hills of the country's geographical center, at an elevation of 1,567 meters (5,141 feet) above sea level. The city is located near the eastern border of Rwanda, about 125 kilometers (78 miles) west of the border with Tanzania and 120 kilometers (75 miles) south of the border with Uganda.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Note that we will be able to select specific OpenAI models \n",
    "# If you have a paid account \n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "question = 'Where is Kigali?'\n",
    "output = llm.invoke(question)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381c4a5-5a33-405c-b82a-baacfebe6e56",
   "metadata": {},
   "source": [
    "**EXERCISE-1. Find another model on Hugging Face to try**\n",
    "- Go to [Hugging Face](https://huggingface.co/models)\n",
    "- Search for **Text Generation** LLMs. Note that large models can be hard and take long to run.\n",
    "- Get the model Id\n",
    "- Initialize the model, and ask it a question/prompt as we did with Falcon model above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4edd5e5-44fa-49b0-b150-64a580da8f66",
   "metadata": {},
   "source": [
    "### . Prompt templates\n",
    "Prompt templates are used for creating prompts in a more modular way, so they can be reused and built on. Chains act as the glue in LangChain; bringing the other components together into workflows that pass inputs and outputs between the different components\n",
    "- They are recipes for generating prompts\n",
    "- Flexible and modular\n",
    "- Can contain: instructions, examples, and additional context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a614f93-417e-4185-96c6-dc0b2ae1704d",
   "metadata": {},
   "source": [
    "###  Chat Models\n",
    "Chat Models are a core component of LangChain. A chat model is a language model that uses chat messages as inputs and returns chat messages as outputs (as opposed to using plain text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684669c2-a2f7-4801-b9db-87b702b545e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the best places to visit in Malawi is Lake Malawi. This stunning freshwater lake is known for its crystal-clear waters, beautiful beaches, and diverse marine life. Visitors can enjoy a variety of water activities such as snorkeling, diving, kayaking, and sailing. The lake is also surrounded by national parks and reserves, offering opportunities for wildlife viewing and hiking. Additionally, the friendly local communities and vibrant culture make Lake Malawi a must-visit destination in Malawi.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')\n",
    "\n",
    "# Define a structured chat prompt\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant who knows a lot about Africa.\"),\n",
    "    (\"human\", \"Respond to the question: {question}\")\n",
    "])\n",
    "\n",
    "# Format the messages with a specific question\n",
    "messages = prompt_template.format_messages(\n",
    "    question=\"What is the best place to visit in Malawi?\"\n",
    ")\n",
    "\n",
    "# Generate a response\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8c433-8672-4ae8-9579-66d5201bc657",
   "metadata": {},
   "source": [
    "## 2.2. Managing chat model memory\n",
    "- A key feature of chatbot applications is the ability to have a conversation, where context from the conversation is stored and available for the model to access for later questions or reference.\n",
    "- Memory is important for conversations with chat models; it opens up the possibility of providing follow-up questions, of building and iterating on model responses, and for chatbots to adapt to the user's preferences and behaviors. \n",
    "- Although LangChain allows us to customize and optimize in-conversation chatbot memory, it is still limited by the model's context window. \n",
    "- An **LLM's context window** is the amount of input text the model can consider at once when generating a response, and the length of this window varies for different models. LangChain has a standard syntax for optimizing model memory. \n",
    "\n",
    "There are three LangChain classes for implementing chatbot memory as follows. \n",
    "### The ```ChatMessageHistory``` Class\n",
    "- The ChatMessageHistory class stores the full history of messages between the user and model. By providing this to the model, we can provide follow-up questions and iterate on the response message.\n",
    "- When additional user messages are provided, the model bases its response on the full context stored in the conversation history\n",
    "- We can use different tools to manage memory usage in LLM applications, and we can even integrate external data to give the models even more context. \n",
    "\n",
    "\n",
    "### The ```ConversationBufferMemory``` class\n",
    "- This gives the application a rolling buffer memory containing the last few messages in the conversation. Users can specify the number of messages to store with the size argument, and the application will discard older messages as newer ones are added. \n",
    "- To integrate the memory type into model, we use a special type of chain for conversations: ```ConversationChain```. \n",
    "\n",
    "### The ```ConversationSummaryMemory``` class\n",
    "- Summarizing important points from a conversation can also be a good way of optimizing memory. The ConversationSummaryMemory class summarizes the conversation over time, condensing the information. \n",
    "- This means that the chat model can remember key pieces of context without needing to store and process the entire conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7f3d4-afdd-422c-8688-7a31cb79bb26",
   "metadata": {},
   "source": [
    "### Trying out the ChatMessageHistory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98c8f82-8e9a-4603-811b-c20d034ee6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning LangChain is like assembling a complex puzzle, where each piece represents a different component of language processing that, when connected, reveals a clear picture of powerful AI applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory  \n",
    "# Initialize chat model\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create a simple in-memory chat history\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# Add past messages\n",
    "history.add_ai_message(\"Hi! Ask me anything please.\")\n",
    "history.add_user_message(\"Describe a metaphor for learning LangChain in one sentence.\")\n",
    "\n",
    "# Run the conversation using invoke()\n",
    "response = chat.invoke(history.messages)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1248fd89-60d3-4d49-a276-f419417f8e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning LangChain is like navigating a complex maze, where each turn reveals new pathways to understanding.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question based on the previous messages \n",
    "history.add_user_message(\"Summarize the preceding sentence in fewer words\")\n",
    "\n",
    "\n",
    "response = chat.invoke(history.messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeeaed5-cce7-4261-a852-eee1941c808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question based on the previous messages \n",
    "history.add_user_message(\"Summarize the preceding sentence in fewer words\")\n",
    "\n",
    "\n",
    "response = chat.invoke(history.messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b5fb0-9fbf-4836-94d0-4017efbdfae0",
   "metadata": {},
   "source": [
    "###  Trying out the ConversationBufferMemory\n",
    "For many applications, storing and accessing the entire conversation history isn't technically feasible. In these cases, the messages must be condensed while retaining as much relevant context as possible. One common way of doing this is with a memory buffer, which stores only the most recent messages based on the parameter ```size```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8742c04-33db-42fb-8509-987dee9e61d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/7vm8r4rj2g90lf39dt0pqwbr0000gn/T/ipykernel_43896/3809992047.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n",
      "/var/folders/4k/7vm8r4rj2g90lf39dt0pqwbr0000gn/T/ipykernel_43896/3809992047.py:11: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Describe a language model in one sentence\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Describe a language model in one sentence', additional_kwargs={}, response_metadata={}), AIMessage(content='A language model is an artificial intelligence system designed to understand, generate, and manipulate human language by predicting the next word in a sequence based on the context of the words that came before it.', additional_kwargs={}, response_metadata={})]\n",
      "Human: Describe it again using fewer words\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Describe a language model in one sentence', additional_kwargs={}, response_metadata={}), AIMessage(content='A language model is an artificial intelligence system designed to understand, generate, and manipulate human language by predicting the next word in a sequence based on the context of the words that came before it.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Describe it again using fewer words', additional_kwargs={}, response_metadata={}), AIMessage(content='A language model is an AI that predicts the next word in a sequence to understand and generate human language.', additional_kwargs={}, response_metadata={})]\n",
      "Human: Describe it again in fewer words but at least one word\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Describe a language model in one sentence', additional_kwargs={}, response_metadata={}), AIMessage(content='A language model is an artificial intelligence system designed to understand, generate, and manipulate human language by predicting the next word in a sequence based on the context of the words that came before it.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Describe it again using fewer words', additional_kwargs={}, response_metadata={}), AIMessage(content='A language model is an AI that predicts the next word in a sequence to understand and generate human language.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Describe it again in fewer words but at least one word', additional_kwargs={}, response_metadata={}), AIMessage(content='A language model is an AI that generates human language.', additional_kwargs={}, response_metadata={})]\n",
      "Human: What did I first ask you? I forgot.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You first asked me to describe a language model in one sentence.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_classic.memory import ConversationBufferWindowMemory  # windowed memory\n",
    "\n",
    "# Chat model (set your key via env or pass directly)\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Keep only the last 4 exchanges in memory\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n",
    "\n",
    "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Demo turns\n",
    "buffer_chain.predict(input=\"Describe a language model in one sentence\")\n",
    "buffer_chain.predict(input=\"Describe it again using fewer words\")\n",
    "buffer_chain.predict(input=\"Describe it again in fewer words but at least one word\")\n",
    "answer = buffer_chain.predict(input=\"What did I first ask you? I forgot.\")\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd52d16-a392-4aad-83e7-2e044b6d0c43",
   "metadata": {},
   "source": [
    "**EXERCISE-2. For the ```ConversationBufferMemory```, change the buffer size to 1 or 2 and see what happens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e535976-cabe-4162-b8ca-e84532dc783c",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory\n",
    "For longer conversations, storing the entire memory, or even a long buffer memory, may not be technically feasible. In these cases, a summary memory implementation can be a good option. Summary memories summarize the conversation at each step to retain the key context for the model to use. This works by using another LLM for generating the summaries, alongside the LLM used for generating the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7325752-2d3d-499e-b7ae-31122b1f64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# PLEASE FOLLOW INSTRUCTIONS AND COMPLETE CODE\n",
    "# ==============================================\n",
    "\n",
    "# Use openAI model from earlier as a summary model\n",
    "summary_llm =  YOUR CODE HERE\n",
    "\n",
    "# Complete code below by putting in summary model above\n",
    "memory = ConversationSummaryMemory(llm=summary_llm)\n",
    "\n",
    "# Create a chat model to use in the Conversation chain below (refer\n",
    "# previous cells where we created OpenAI chat model\n",
    "chat_model = YOUR CODE HERE\n",
    "\n",
    "# Create a conversation chain as we did before \n",
    "summary_chain = YOUR CODE HERE\n",
    "\n",
    "summary_chain.predict(input=\"Please tell me about Malawi.\")\n",
    "summary_chain.predict(input=\"Does that affect Malawi's income?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c250ad-6f8a-42a6-99d5-bdb7b75df1e1",
   "metadata": {},
   "source": [
    "# 3. Adding External Documents to LLMs\n",
    "As mentioned in the lectures, LLMs are trained on a specific dataset (often publicly available internet data) up to some point in time. Therefore, if you have some custom organization documents or data, the LLMs will not be able to provide answers based on that information. Furthermore, if there is any new information which came after the LLM was trained, the LLM will not have that information either. \n",
    "\n",
    "The main remedy to deal with this is to provide the LLM with external documents. Adding external documents further helps with **hallucinations** as the LLM has little opportunity to make up stuff (hallucinate) when it has access to this extra knowledge.\n",
    "\n",
    "In LangChain, there are three main steps to provide external documents to the LLM (essentially create a Retrieval Augmented Generation)-**RAG Chatbot**\n",
    "1. Identify the data sources (documents, datasets, websites, databases etc).\n",
    "\n",
    "2. Load the documents into LangChain using document loaders. LangChain can work with different document sources, please see [the documentation](https://python.langchain.com/v0.1/docs/integrations/document_loaders/). \n",
    "\n",
    "3. Splitting the documents into chunks. \n",
    "\n",
    "4. Create vector embeddings and store into a vector database for retrievval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4a208-8c25-4060-bf4b-c4eb06e26557",
   "metadata": {},
   "source": [
    "### 3.1 Document Loaders\n",
    "LangChain has more than 160 document loaders. Some loaders are provided by 3rd parties who manage unique document formats. These include Amazon S3, Microsoft, Google Cloud, Jupyter notebooks, pandas DataFrames, unstructured HTML, YouTube audio transcripts, and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efebd7e-59c0-4aef-81a3-5d9a500d1319",
   "metadata": {},
   "source": [
    "#### PDF Document Loader\n",
    "- Requires installation of the ```pypdf``` package as a dependency.\n",
    "- There are many different types of PDF loaders in LangChain, and there is documentation available online for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21c844-25b1-4447-a7ea-4aff7ad450ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c534c-4a01-49a4-9c8f-16450dec011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(str(FILE_VISIT_RW))\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6fbaf-34c0-4fc6-8570-76aa853e78a5",
   "metadata": {},
   "source": [
    "**EXERCISE-3. Explore other LangChain Loaders**\n",
    "\n",
    "Check the LangChain [document loaders documentation](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.document_loaders).\n",
    "1. Identify 5 document loaders you find interesting. What are third party document loaders?\n",
    "2. **HTML loaders**. Explore the html or webpage loaders. \n",
    "3. Pick one of your favourite webpages and load it using the ```UnstructuredHTMLLoader``` loader module.\n",
    "4. How do you think this changes your approach to ```web-scraping```. Do you think web scraping will change or not with this new capabilities to just connect to a website and query it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bde0cf-56cb-4726-a3f0-cfc839ba1d3e",
   "metadata": {},
   "source": [
    "### 3.2 Preparing documents for vector database and retrieval\n",
    "In this stage, there are two sub-steps:\n",
    "- The document is split to enhance efficiency in storage, indexing and ultimately efficient retrieval. Furthermore, chunking also helps with ensuring the document (which act as context) can fit in the context window \n",
    "- An embedding model is used to convert the documents into ```vector embeddings```\n",
    "- The vectorized data is stored into a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba116fde-3896-40d2-b921-18c2de13b56d",
   "metadata": {},
   "source": [
    "#### Splitting/Chunking Documents\n",
    "- Given a PDF document, one naive splitting option would be to separate the document into lines as they appear in the document. This would be simple to implement but could be problematic. Key context required for understanding one line is often found in a different line, and these lines would be processed separately, so we need another strategy which can maintain context across pieces of texts in the document-enter the **overlap concept**.\n",
    "We will compare two document splitting methods from LangChain. \n",
    ">- **CharacterTextSplitter** splits text based on a specified separator, looking at individual characters. This method splits based on the separator first, then evaluates chunk size and chunk overlap.\n",
    ">- **RecursiveCharacterTextSplitter** attempts to split by several separators recursively until the chunks fall within the specified chunk size. There are many other methods that use natural language processing to infer meaning and split appropriately. Optimizing this is an active area of research.\n",
    "\n",
    "There isn't one strategy that works for all situations when it comes to splitting documents. \n",
    "It's often the case of experimenting with multiple methods, and seeing which one strikes the right balance between retaining sufficient context and managing chunk size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a6334-4e51-4e28-b2a9-1967fd36d6b7",
   "metadata": {},
   "source": [
    "##### CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabe5b1-557f-4480-83ef-9637682546c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "quote = 'One machine can do the work of fifty ordinary humans.\\\n",
    "No machine can do the work of one extraordinary human.'\n",
    "\n",
    "chunk_size = 24\n",
    "chunk_overlap = 3\n",
    "\n",
    "ct_splitter = CharacterTextSplitter(\n",
    "    separator=\".\", \n",
    "    chunk_overlap=chunk_overlap, \n",
    "    chunk_size=chunk_size\n",
    ")\n",
    "\n",
    "docs = ct_splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08337229-f503-429b-8345-e5d987f0d774",
   "metadata": {},
   "source": [
    "##### RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f2615-19d8-45a7-b626-f78e45332534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same variables: chunk_size and chunk_overlap, instatiate RecursiveCharacterTextSplitter\n",
    "rc_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap = chunk_overlap)\n",
    "docs = rc_splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8777067d-35d7-471d-983f-fec0a6aafbc0",
   "metadata": {},
   "source": [
    "#### Load data into a vector database\n",
    "At this stage, you will be faced with a decision to choose which vector database to use. \n",
    "For our simple demonstration purpose, we will use [chromadb](https://www.trychroma.com), an open source vector database solution. The type of vector database solution you choose can depend on numerous factors such as:\n",
    "- How large are the documents you will be processing\n",
    "- How much money you have to spend on the project\n",
    "- Efficiency/latency requirements for your use case, if you need to provide solution in real-time/fast, you may need a different solution\n",
    "- Accuracy requirements. Sometimes there is a tradeoff between accuracy and latecy.\n",
    "- Integration requirements with existing platforms. In somecases, people use ```PostgreSQL``` because they are already using it and it has enough add on extensions for vector database capabilities.\n",
    "\n",
    "Another decision choice is the **embedding model**- the LLM which converts the text/documents into vectors. There are many options on the market and the choice comes down to things such as:\n",
    "- Available budget\n",
    "- Compatibility with the LLM you are using in the generation phase. People do use a different embedding model from the generation model\n",
    "> embedding_llm = Mistral, \n",
    "> chat_model = ChatOpenAI\n",
    "- Nature of documents, size and alot of other factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab4154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4c76d-7248-49d6-acf2-3c1193bd2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# Lets load the Cholera paper and then store it in a database\n",
    "loader = PyPDFLoader(str(FILE_VISIT_RW))\n",
    "data = loader.load()\n",
    "\n",
    "chunk_size = 100\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Split with RecursiveCharacterTextSplitter\n",
    "rc_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap = chunk_overlap)\n",
    "docs = rc_splitter.split_documents(data)\n",
    "\n",
    "# Lets use openAI embedding model\n",
    "#embedding_model = OpenAIEmbeddings(openai_api_type=OPENAI_API_KEY)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Directory to store our database-set this to the data directory\n",
    "vectordb = Chroma(persist_directory=str(DIR_DATA), embedding_function=embedding_model)\n",
    "\n",
    "# Store the databse\n",
    "vectordb.persist()\n",
    "\n",
    "# Create the database\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30d865a-42fb-4325-b53c-84da656a0703",
   "metadata": {},
   "source": [
    "**EXERCISE-4. Explore what functionality is available under the database object ```docstorage_cholera```**\n",
    "- You can use ```dir(object)``` to check available attributes and functions\n",
    "- Note that there many search related functions which enables you to control how user queries are searcherd when building Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea226f-8de3-4c5c-a69e-9d8971b047cf",
   "metadata": {},
   "source": [
    "### 3.3 Retrieval\n",
    "Now that we have added our external file. Lets use the added document as context in our LLM chains and ask questions again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5276c-4235-44c8-926f-652acf3d16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "# Create LLM as before \n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create retriever with \n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# The question we will ask the LLM\n",
    "# You can ask these questions in French and LLM will also answer in French\n",
    "question = \"Where are lakes Ihema and Rwanyakazinga located?\"\n",
    "\n",
    "# Answer without RAG\n",
    "output = llm.invoke(question)\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"LLM Output without using RAG-external document from WHO website\")\n",
    "print(\"=\"*60)\n",
    "print(output)\n",
    "\n",
    "# For RAG Chain, we put in the question as dictionary\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"LLM Output with RAG-external document from WHO website\")\n",
    "print(\"=\"*60)\n",
    "print(qa.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c735436-9e4a-410d-8f37-4f290cf51e1b",
   "metadata": {},
   "source": [
    "**EXERCISE-5. Implement a simple RAG as we did above**\n",
    "1. Use the ```Dengue-Global-situation``` file to create a new Chroma database\n",
    "2. Implement a RAG chainas we did above.\n",
    "3. Compare answers between a the LLM with RAG and no RAG\n",
    "\n",
    "**Hint.** Copy and paste the code from above and edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375cfcc-c6a9-406a-8e72-cd0a04a9b2ac",
   "metadata": {},
   "source": [
    "### 3.4 Retrieval with sources reference\n",
    "In reallife applications, you will have hundreds or thousands of documents. A user of your system may need to know the spurce of the answrs they are getting. Most RAG systems are able to provide details of where the information is coming from. For example, in the RAG-Malawi example, the RAG system can provide the page numbers. In this case, with LangChain, you can you can just provide information about the document where the answer came from.\n",
    "\n",
    "One method of mitigating the risk of LLM hallucinations from RAG is using RetrievalQAWithSourcesChain, which also returns the data source of the answer. Aside from the chain class, the code is exactly the same as RetrievalQA. However, this class returns a dictionary containing a 'sources' key and an 'answer' key. The 'sources' key refers to the file where the answer came from, which is helpful when there are many documents in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ddc48-1e99-43a4-ae28-298d5427b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "results = qa({\"question\": \"Are there any disease outbreaks in Chad?\"},\n",
    "             return_only_outputs=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0f265-8583-4357-9040-8dd75429179c",
   "metadata": {},
   "source": [
    "# 4. LangChain Expression Language (LCEL)\n",
    "> In summary, LCEL is a different (recommended) syntax of achieving the same things we have done in LangChain\n",
    "\n",
    "LCEL is a key part of the LangChain toolkit. We can use it to connect prompts, models, and retrieval components using a **pipe (|)** operator rather than task-specific classes. It also lets us create complex workflows that work well in production environments. These chains have built-in support for batch processing, streaming, and asynchronous execution. This makes it easy to integrate with other LangChain tools and utilities like **LangSmith** and **LangServe**.\n",
    "\n",
    "A few notes about the chain with LCEL\n",
    "- The ```| (pipe)``` in LCEL indicates that the output from one component will be used as the input to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229521e-790e-41b8-9fd8-159a54cae8c7",
   "metadata": {},
   "source": [
    "## 4.1 A Simple Chain with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efb378-36b2-4499-a033-c3145101475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "prompt = ChatPromptTemplate.from_template(\"You are a helpful personal assistant. \\\n",
    "Answer the following question: {question}\")\n",
    "\n",
    "# Create Chain in LCEL fashion\n",
    "llm_chain = prompt | model\n",
    "\n",
    "# Recall how we created a chain before \n",
    "#llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "\n",
    "# Run using invoke\n",
    "print(llm_chain.invoke(\"What is the capital of Tunisia?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8949f-0ad0-4d22-8dbf-e8fd992f4065",
   "metadata": {},
   "source": [
    "## 4.2 RAG with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b9b2eb-db18-49e2-b75e-2c27fb862f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Delete the existing Chroma database\n",
    "chroma_dir = \"./chroma_db\"\n",
    "if os.path.exists(chroma_dir):\n",
    "    shutil.rmtree(chroma_dir)\n",
    "    print(\"✅ Deleted existing Chroma database\")\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create NEW vectorstore\n",
    "vectorstore = Chroma.from_texts(\n",
    "    [\"Dunstan stayed in Tunis, the capital of Tunisia from Sunday May 26 to Saturday May 31.\"],\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=chroma_dir  # Specify directory\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"Answer the question based on the context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create chain\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test\n",
    "result = chain.invoke(\"When did Dunstan visit Tunisia?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac7202-4bb2-446d-85bf-e55bed0f53b1",
   "metadata": {},
   "source": [
    "## 4.3 More things you can do with LCEL\n",
    "There are alot of things you can do with LCEL. For example,\n",
    "- **Batch or Streaming**. LCEL chains can be run in ```batch``` mode or ```streaming``` mode\n",
    "- **Sequential chains.**. Sequential chains utilize step-by-step processing of inputs, where the output from one step becomes the input for the next. This enables a clear and organized flow of information within the chain. They provide flexibility in constructing custom pipelines by combining different components, such as prompts, models, retrievers, and output parsers, to suit specific use cases and requirements.\n",
    "- **Passing Data Across Chains.** There are many cases where your application will require the use of several chains that pass outputs between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bc2fc-cb3b-4d23-abab-52451461e0c4",
   "metadata": {},
   "source": [
    "### Using sequential chaining to create Python code and check it with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0c20c-167c-454c-a87a-db01aa8f2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Write Python code to loop through the following list, printing each element: {list}\"\"\")\n",
    "validate_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Consider the following Python code: {answer} If it doesn't use a list comprehension, update it to use one. If it does use a list comprehension, return the original code without explanation:\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create the sequential chain\n",
    "chain = ({\"answer\": coding_prompt | llm | StrOutputParser()}\n",
    "         | validate_prompt\n",
    "         | llm \n",
    "         | StrOutputParser() )\n",
    "\n",
    "# Invoke the chain with the user's question\n",
    "print(chain.invoke({\"list\": \"[3, 1, 4, 1]\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9fb57-5565-4bdf-a977-67f735260e51",
   "metadata": {},
   "source": [
    "# 5. LangChain Agents\n",
    "In LLMs and Gen AI, the idea behind agents is to use language models to determine which a sequence of actions to take to meet a pre-defined objective. Thus, the LLM is able solve complex problems or perform complex tasks by planning, determing what tools to use and what knowledge to get until the task is solved without explicit supervision.\n",
    "\n",
    "- Agents often use tools, which, in LangChain, are functions used by the agent to interact with the system. These tools can be high-level utilities to transform inputs, or they can be specific to a series of tasks. Agents can even use chains and other agents as tools!\n",
    "- In LangChain, there different agent types. See [this documentation](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/) for explanation of how the agents are categorized. \n",
    "## Components of a LangChain Agent\n",
    "There are four primary components to LangChain agents. \n",
    "- The user input in the form of a prompt represents the initial input provided by the user. \n",
    "- The definition for handling the intermediate steps explains how to handle and process actions during the agent's execution. \n",
    "- The agent also needs to have a definition for the tools and model behavior to execute. \n",
    "- The output parser formats the output generated by the model into the most appropriate format for the use case. Agents can be defined for specificity or high-level thought processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa06af-c231-4237-8d62-dc42ff0f59de",
   "metadata": {},
   "source": [
    "## 5.1 Zero-Shot ReAct agent\n",
    "ReAct stands for **Reasoning and Acting**. This simplifies the answer to infer as much context as possible. \n",
    "We start by importing the initialize_agent function and AgentType for agent creation and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e61715",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3cf87-c98c-4dfa-9ab4-65bdee6f47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "# Define LLM (parameter changed from openai_api_key to api_key)\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "# Define tools\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run\n",
    "result = agent.run(\"What is 10 multiplied by 50?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0fb85-881a-4b00-b1a6-1cee7d7a3f75",
   "metadata": {},
   "source": [
    "## 5.2 Other Agents \n",
    "There are alot of other agents and tools in LangChain. For example, in order to interact with a database or structured dataset we will utilise an ```SQLAgent```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ae0ee-c01d-4fea-b992-110d6c7e0edf",
   "metadata": {},
   "source": [
    "# 6. Evaluating LLM Outputs in LangChain\n",
    "As mentioned in Lectures, its important to evaluate LLM model outputs as well as all ML based outputs fot that matter.  \n",
    "Although Gen AI may seem very smart, the models still make alot of mistakes. As such, evaluating AI applications is important for several reasons. \n",
    "- First, it checks if the AI model can accurately interpret and respond to a variety of inputs. This is vital in applications where responses inform decision-making, and reliability is paramount. \n",
    "- Evaluation also help identify the strengths and weaknesses of a model, which allows for targeted and continuous improvements, and builds trust among users and stakeholders. \n",
    "- Evaluation allows us to re-align model output with human intent, getting to the ideal responses faster.\n",
    "\n",
    "## LangChain evaluation tools\n",
    "LangChain has built-in evaluation tools for comparing model outputs based on common criteria, such as relevance and correctness. It also provides tools for defining custom criteria, which we can tailor to specific use cases. Finally, the ```QAEvalChain class``` is another tool that can be used to measure how well an AI's response answers a specific question using ground truth responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf96d76-5901-4fe0-83b5-881e6e340b92",
   "metadata": {},
   "source": [
    "## 6.1 LangChain Built-in Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7752f75-65d4-4a91-83dc-3b4740e544d9",
   "metadata": {},
   "source": [
    "**EXERCISE-6: Explore Evalution Metrics in LangChain**\n",
    "- run this import statement: ```from langchain.evaluation import Criteria```\n",
    "- use ``list`` function pn Criteria to check the list of available functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44f6f3-2489-4670-88f4-a3361c6a7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "\n",
    "# Initialize LLM (parameter changed from openai_api_key to api_key)\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Or use ChatOpenAI (recommended)\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Load evaluator\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"relevance\", llm=llm)\n",
    "\n",
    "# Evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"The capital of New York state is Albany\",\n",
    "    input=\"What is 26 + 43?\"\n",
    ")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2c168-9b70-427d-9e0b-231212ec7699",
   "metadata": {},
   "source": [
    "**EXERCISE-7: Try doing the same evaluation above with a different LLM (e.g., Mistral)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53d768-a26c-4509-948e-c464ebd20310",
   "metadata": {},
   "source": [
    "## 6.2 Defining Custom Metrics\n",
    "To customize the criteria, we need to evaluate the specific use case and define a dictionary named custom_criteria. This example adds simplicity, bias, clarity, and truthfulness criteria. Custom criteria work by mapping criteria names to the questions that are used to evaluate the strings. To use these new criteria, create an evaluator object, but this time, using our custom_critera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b248976-85a5-4ef3-9998-48cf1afa9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_criteria = {\"simplicity\": \"Does the language use brevity?\",\n",
    "                   \"bias\": \"Does the language stay free of human bias?\",\n",
    "                   \"clarity\": \"Is the writing easy to understand?\",\n",
    "                   \"truthfulness\": \"Is the writing honest and factual?\"}\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", criteria=custom_criteria,\n",
    "                           llm=llm)\n",
    "eval_result = evaluator.evaluate_strings(input=\"What is the best Italian restaurant in New York City?\",\n",
    "prediction=\"That is a subjective statement and I cannot answer that.\")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61e633-de72-4019-940c-7021b0e7c2e1",
   "metadata": {},
   "source": [
    "## 6.3 QAEvalChain\n",
    "Question-Answering (QA) is one of the most popular applications LLMs. But it is often not always obvious to determine what parameters (e.g., chunk size) or components (e.g., model choice, VectorDB) yield the best QA performance in the system we are building. The QA eval chain is an LLM chain for evaluting performance of an LLM on QA task. Refer to this detailed [LangChain blog post](https://blog.langchain.dev/auto-eval-of-question-answering-tasks/) for details about QAEvalChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585ea3a-1be2-40db-9c88-774c03e220a7",
   "metadata": {},
   "source": [
    "### 6.3.1 Trying out QAEvalChain\n",
    "As a metric, QAEvalChain focuses on the **accuracy** and **relevance** of the response. In this chain, RAG will be used to store the document and ground truth responses, and an evaluation model instance is used to compare the semantic meaning of a model's results with the ground truth. \n",
    "\n",
    "First, we load our data source, in this case, a PDF document, and split it into chunks. Next, we set up the embeddings model, vector database, and LLM, and combine them in a chain. The input_key is set to \"question\", as questions will be used to query the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59bac0-ed4c-4ffe-b64c-423f88f1aab3",
   "metadata": {},
   "source": [
    "### Create a RAG Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc52a82-7b90-4b0f-953a-32cb90beee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "# Delete the existing Chroma database FIRST\n",
    "chroma_dir = str(DIR_DATA)  # or \"./chroma_db\"\n",
    "if os.path.exists(chroma_dir):\n",
    "    shutil.rmtree(chroma_dir)\n",
    "    print(\"✅ Deleted existing Chroma database\")\n",
    "\n",
    "# Set API key (recommended way)\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(str(FILE_VISIT_RW))\n",
    "data = loader.load()\n",
    "\n",
    "# Split documents\n",
    "chunk_size = 100\n",
    "chunk_overlap = 50\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "docs = rc_splitter.split_documents(data)\n",
    "\n",
    "print(f\"📄 Loaded {len(data)} pages, split into {len(docs)} chunks\")\n",
    "\n",
    "# Initialize OpenAI embedding model (parameter changed)\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create the vectorstore from documents\n",
    "docstorage = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=chroma_dir,\n",
    "    collection_name=\"visit_rw_docs\"\n",
    ")\n",
    "\n",
    "print(\"✅ Vectorstore created and persisted\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "# Create retriever\n",
    "retriever = docstorage.as_retriever()\n",
    "\n",
    "# Define the retrieval QA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test it\n",
    "question = \"What is the main topic of the document?\"\n",
    "result = qa.invoke({\"query\": question})\n",
    "\n",
    "print(f\"\\n❓ Question: {question}\")\n",
    "print(f\"✅ Answer: {result['result']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dddef3-e7ab-4c3e-9086-121be7b3b8a4",
   "metadata": {},
   "source": [
    "## Define a Question Set as Key-Value Pairs in a Dict\n",
    "This is a ground-truth dataset which a list of questions and their correct responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d95da4-f130-4a2f-b0e5-40d22146674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_set = [{\"question\": \"Did dengue cases increase in 2023?\",\n",
    "                 \"answer\": \"Yes, in 2023, there was an increase in cases globally.\"},\n",
    "                {\"question\": \"According to the document, which are the top four regions affected by arboviral diseases?\",\n",
    "                \"answer\": \"Africa is oe of the top four regions\"},\n",
    "                {\"question\": \"How is dengue virus transimitted to humans?\",\n",
    "                 \"answer\": \"through the bite of infected mosquitoes\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a509f17-4bec-4bdc-805e-321b37581a84",
   "metadata": {},
   "source": [
    "## Run QAEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff29e4-037b-464f-9135-3f311daf4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.evaluation import QAEvalChain\n",
    "predictions = qa.apply(question_set)\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "results = eval_chain.evaluate(question_set,predictions, question_key=\"question\",prediction_key=\"result\", answer_key='answer')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20cb10-2438-43b2-b9d0-df3ae73d6418",
   "metadata": {},
   "source": [
    "**EXERCISE-7 (Do this in Your Groups): Run Evaluation on a Custom Eval Dataset for a RAG Chatbot QA Task**\n",
    "1. Create a RAG LLM Chain as we have done before.\n",
    "Please identify a PDF document to use which contains some new information that the LLMs do not have. \n",
    "Note that it can be a French or English document.\n",
    "2. Create 5 pairs of questions and correct answers to use to evaluate your RAG\n",
    "3. Run QAEVAL on the eval dataset and report how many responses did the LLM get correct.\n",
    "4. Do this again with a different LLM (e.g., Falcon or Mistral) and compare performance across models. *Note that your eval dataset remains the same.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20065378-5cd5-4b0f-ae9a-c2869075441a",
   "metadata": {},
   "source": [
    "# 7. Summary\n",
    "-----\n",
    "In this notebook, we covered the basics of how to use LangChain to interact with both proprietary models from OpenAI and open source LLMs through Hugging Face library. We noted that there are two approaches to building Chains with LangChain: either using the functions or using the LCEL syntax. We covered key topics as follows: creating chains and interacting with LLMs; managing memeory of chat models; setup a RAG based chains which incorprates external documents and evaluating LLM outputs. \n",
    "\n",
    "What we have covered in this notebook is the tip of the ice-berg just to get you started on building LLM based applications with LangChain and other tools. There are alot of other things to learn and check.\n",
    "- What are other frameworks whoch perform the same tasks as LangChain?\n",
    "- LangChain Agents and LLM agents in general\n",
    "- Vector databases and their role \n",
    "- How to work with different document sources (e.g., websites)\n",
    "- How to choose embedding models and the influence they have on generation\n",
    "- Which model to use: instruct/chat/text generation\n",
    "- and more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ee5b1-8133-4405-98fc-e56057daece6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
