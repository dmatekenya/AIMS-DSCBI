{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Large Language Models (LLMs)\n",
    "## A Beginner's Guide to Next Token Prediction, Tokenization, and Embeddings\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand how LLMs predict the next token\n",
    "2. Learn about tokenization and how it works across different languages\n",
    "3. Build intuition about vector embeddings and how meaning is represented\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python knowledge\n",
    "- Understanding of basic machine learning concepts (helpful but not required)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required libraries. We'll use open-source models from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch numpy matplotlib seaborn scikit-learn tokenizers sentencepiece --quiet\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Next Token Prediction - The Core of LLMs\n",
    "\n",
    "## What is Next Token Prediction?\n",
    "\n",
    "LLMs work by predicting the next token (word or subword) given a sequence of previous tokens. This simple idea is the foundation of how models like GPT, LLaMA, and others generate text.\n",
    "\n",
    "**Key Concept:** Given \"The cat sat on the\", the model predicts \"mat\" (or \"chair\", \"floor\", etc.) based on probabilities.\n",
    "\n",
    "Let's see this in action with GPT-2, a small open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 small model (124M parameters)\n",
    "print(\"Loading GPT-2 model...\")\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "print(f\"Model size: ~124M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Next Token Prediction\n",
    "\n",
    "Let's see what the model predicts as the next token for different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_tokens(text, top_k=10):\n",
    "    \"\"\"\n",
    "    Predict the most likely next tokens given input text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text prompt\n",
    "        top_k: Number of top predictions to show\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        predictions = outputs.logits\n",
    "    \n",
    "    # Get the predictions for the next token (last position)\n",
    "    next_token_logits = predictions[0, -1, :]\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # Get top k predictions\n",
    "    top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìù Input: '{text}'\\n\")\n",
    "    print(\"Top predictions for the next token:\\n\")\n",
    "    print(f\"{'Rank':<6} {'Token':<20} {'Probability':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for rank, (prob, idx) in enumerate(zip(top_probs, top_indices), 1):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"{rank:<6} {repr(token):<20} {prob.item():.4f} ({prob.item()*100:.2f}%)\")\n",
    "    \n",
    "    return top_probs, top_indices\n",
    "\n",
    "# Example 1: Simple completion\n",
    "predict_next_tokens(\"The capital of Rwanda is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Another completion\n",
    "predict_next_tokens(\"Once upon a time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Technical context\n",
    "predict_next_tokens(\"Machine learning is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 1: Experiment with Next Token Prediction\n",
    "\n",
    "Try different prompts and observe:\n",
    "1. How do probabilities change with different contexts?\n",
    "2. What happens with ambiguous prompts?\n",
    "3. Try prompts in different languages (if the model supports them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Try your own prompts here:\n",
    "your_prompt = \"The weather today is\"  # Change this!\n",
    "predict_next_tokens(your_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Probability Distribution\n",
    "\n",
    "Let's visualize how confident the model is about different predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(text, top_k=15):\n",
    "    \"\"\"\n",
    "    Visualize the probability distribution of next token predictions.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "    tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(range(top_k), top_probs.numpy())\n",
    "    plt.yticks(range(top_k), [f\"{i+1}. {repr(t)}\" for i, t in enumerate(tokens)])\n",
    "    plt.xlabel('Probability')\n",
    "    plt.title(f'Top {top_k} Next Token Predictions for: \"{text}\"')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "visualize_predictions(\"The capital of Rwanda is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Temperature in Text Generation\n",
    "\n",
    "Temperature controls the randomness of predictions. Let's see how it affects generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temperature(prompt, temperature=1.0, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate text with different temperature settings.\n",
    "    \n",
    "    Temperature:\n",
    "    - Low (0.1-0.5): More deterministic, focused\n",
    "    - Medium (0.7-1.0): Balanced\n",
    "    - High (1.5-2.0): More random, creative\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Temperature: {temperature}\")\n",
    "    print(f\"Generated: {generated_text}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "prompt = \"Artificial intelligence will\"\n",
    "\n",
    "print(\"Comparing different temperatures:\\n\")\n",
    "generate_with_temperature(prompt, temperature=0.3)\n",
    "generate_with_temperature(prompt, temperature=1.0)\n",
    "generate_with_temperature(prompt, temperature=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Tokenization - Breaking Text into Pieces\n",
    "\n",
    "## What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens) that the model can process. Different languages and writing systems require different tokenization strategies.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Tokens can be words, subwords, or characters\n",
    "- Different tokenizers handle different languages differently\n",
    "- Languages with rich morphology (like Kinyarwanda) may be tokenized less efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Different Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different tokenizers\n",
    "print(\"Loading different tokenizers...\\n\")\n",
    "\n",
    "tokenizers_to_compare = {\n",
    "    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"BERT\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    \"RoBERTa\": AutoTokenizer.from_pretrained(\"roberta-base\"),\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Tokenizers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenization(text, tokenizers_dict):\n",
    "    \"\"\"\n",
    "    Compare how different tokenizers process the same text.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìù Original text: '{text}'\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name, tokenizer in tokenizers_dict.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Number of tokens: {len(tokens)}\")\n",
    "        print(f\"  Tokens: {tokens}\")\n",
    "        print(f\"  Token IDs: {token_ids}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Example 1: English text\n",
    "compare_tokenization(\"Hello, how are you today?\", tokenizers_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Technical text\n",
    "compare_tokenization(\"Machine learning is revolutionizing technology.\", tokenizers_to_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization for Different Languages\n",
    "\n",
    "Let's see how tokenization works for different languages, including Kinyarwanda. This is important because most tokenizers are trained primarily on English data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences in different languages\n",
    "multilingual_examples = {\n",
    "    \"English\": \"Hello, how are you?\",\n",
    "    \"Kinyarwanda\": \"Mwaramutse, mumeze mute?\",\n",
    "    \"French\": \"Bonjour, comment allez-vous?\",\n",
    "    \"Swahili\": \"Habari, unajisikiaje?\",\n",
    "    \"Spanish\": \"Hola, ¬øc√≥mo est√°s?\",\n",
    "}\n",
    "\n",
    "def analyze_multilingual_tokenization(examples, tokenizer, tokenizer_name):\n",
    "    \"\"\"\n",
    "    Analyze how a tokenizer handles different languages.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Tokenizer: {tokenizer_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for language, text in examples.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        num_tokens = len(tokens)\n",
    "        num_chars = len(text)\n",
    "        efficiency = num_chars / num_tokens if num_tokens > 0 else 0\n",
    "        \n",
    "        results[language] = {\n",
    "            'tokens': tokens,\n",
    "            'num_tokens': num_tokens,\n",
    "            'num_chars': num_chars,\n",
    "            'efficiency': efficiency\n",
    "        }\n",
    "        \n",
    "        print(f\"{language}:\")\n",
    "        print(f\"  Text: '{text}'\")\n",
    "        print(f\"  Tokens: {tokens}\")\n",
    "        print(f\"  Number of tokens: {num_tokens}\")\n",
    "        print(f\"  Characters per token: {efficiency:.2f}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze with GPT-2 tokenizer\n",
    "gpt2_results = analyze_multilingual_tokenization(\n",
    "    multilingual_examples, \n",
    "    tokenizers_to_compare[\"GPT-2\"],\n",
    "    \"GPT-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tokenization efficiency across languages\n",
    "def visualize_tokenization_efficiency(results):\n",
    "    \"\"\"\n",
    "    Visualize how efficiently different languages are tokenized.\n",
    "    \"\"\"\n",
    "    languages = list(results.keys())\n",
    "    num_tokens = [results[lang]['num_tokens'] for lang in languages]\n",
    "    efficiency = [results[lang]['efficiency'] for lang in languages]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Number of tokens\n",
    "    ax1.bar(languages, num_tokens, color='steelblue')\n",
    "    ax1.set_ylabel('Number of Tokens')\n",
    "    ax1.set_title('Number of Tokens per Language')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Efficiency (chars per token)\n",
    "    ax2.bar(languages, efficiency, color='coral')\n",
    "    ax2.set_ylabel('Characters per Token')\n",
    "    ax2.set_title('Tokenization Efficiency (Higher = More Efficient)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_tokenization_efficiency(gpt2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Explore Tokenization\n",
    "\n",
    "### Part A: Experiment with Different Texts\n",
    "\n",
    "Try tokenizing:\n",
    "1. Long Kinyarwanda sentences\n",
    "2. Technical terms in Kinyarwanda\n",
    "3. Mixed language text (code-switching)\n",
    "\n",
    "**Questions to consider:**\n",
    "- Which languages are tokenized more efficiently?\n",
    "- Why might some languages require more tokens?\n",
    "- What are the implications for LLM performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Add your own examples\n",
    "your_examples = {\n",
    "    \"Example 1\": \"Add your text here\",\n",
    "    \"Example 2\": \"Add another example\",\n",
    "    # Add more examples\n",
    "}\n",
    "\n",
    "# Uncomment to test:\n",
    "# your_results = analyze_multilingual_tokenization(your_examples, tokenizers_to_compare[\"GPT-2\"], \"GPT-2\")\n",
    "# visualize_tokenization_efficiency(your_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: OpenAI Tokenizer Playground\n",
    "\n",
    "**üìé Online Exercise:**\n",
    "\n",
    "Visit the OpenAI Tokenizer Playground: https://platform.openai.com/tokenizer\n",
    "\n",
    "**Tasks:**\n",
    "1. Test the same Kinyarwanda sentences you used above\n",
    "2. Compare the token counts with GPT-2\n",
    "3. Try different GPT models (GPT-3.5, GPT-4) and observe differences\n",
    "4. Experiment with:\n",
    "   - Punctuation\n",
    "   - Numbers\n",
    "   - Special characters\n",
    "   - Emojis\n",
    "\n",
    "**Discussion Points:**\n",
    "- Why do newer models (GPT-4) tokenize some languages more efficiently?\n",
    "- What does this mean for cost and performance?\n",
    "- How might this affect model training on low-resource languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Subword Tokenization\n",
    "\n",
    "Let's visualize how subword tokenization works with a detailed example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_subword_tokens(text, tokenizer, tokenizer_name):\n",
    "    \"\"\"\n",
    "    Visualize how text is broken into subword tokens.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    print(f\"\\nTokenizer: {tokenizer_name}\")\n",
    "    print(f\"Original text: '{text}'\")\n",
    "    print(f\"\\nToken breakdown:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, token in enumerate(tokens, 1):\n",
    "        # Show the token and its representation\n",
    "        token_clean = token.replace('ƒ†', '‚ñÅ')  # Show spaces as ‚ñÅ\n",
    "        token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
    "        print(f\"Token {i:2d}: {token_clean:20s} (ID: {token_id})\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total tokens: {len(tokens)}\\n\")\n",
    "\n",
    "# Example with uncommon/technical words\n",
    "examples = [\n",
    "    \"The biotechnology industry is growing.\",\n",
    "    \"Umunyarwanda w'umwanditsi\",  # Kinyarwanda\n",
    "    \"Preprocessing and tokenization\",\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    visualize_subword_tokens(example, tokenizers_to_compare[\"GPT-2\"], \"GPT-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Vector Embeddings - Representing Meaning\n",
    "\n",
    "## What are Embeddings?\n",
    "\n",
    "Embeddings are numerical representations (vectors) of tokens that capture their meaning. Similar words have similar embeddings.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Each token is represented as a vector of numbers (typically 768 or 1024 dimensions)\n",
    "- Similar meanings ‚Üí Similar vectors\n",
    "- We can measure similarity using cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Embeddings from GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Get the embedding vector for a word.\n",
    "    \"\"\"\n",
    "    # Get token ID\n",
    "    token_id = tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "    \n",
    "    # Get embedding from model's embedding layer\n",
    "    embedding = model.transformer.wte.weight[token_id].detach().numpy()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Get embeddings for some words\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"cat\", \"dog\", \"computer\", \"phone\"]\n",
    "embeddings = {}\n",
    "\n",
    "for word in words:\n",
    "    embeddings[word] = get_word_embedding(word, model, tokenizer)\n",
    "    print(f\"‚úÖ Embedding for '{word}': shape {embeddings[word].shape}\")\n",
    "\n",
    "print(f\"\\nEmbedding dimension: {embeddings[words[0]].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Similarity Between Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(words, embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between all pairs of words.\n",
    "    \"\"\"\n",
    "    n = len(words)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            emb1 = embeddings[word1].reshape(1, -1)\n",
    "            emb2 = embeddings[word2].reshape(1, -1)\n",
    "            similarity_matrix[i, j] = cosine_similarity(emb1, emb2)[0, 0]\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def visualize_similarity_matrix(words, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Visualize the similarity matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, \n",
    "                xticklabels=words, \n",
    "                yticklabels=words,\n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                cmap='coolwarm',\n",
    "                center=0.5,\n",
    "                vmin=0,\n",
    "                vmax=1)\n",
    "    plt.title('Cosine Similarity Between Word Embeddings')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute and visualize similarities\n",
    "similarity_matrix = compute_similarity_matrix(words, embeddings)\n",
    "visualize_similarity_matrix(words, similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Similarity Scores\n",
    "\n",
    "**What do the numbers mean?**\n",
    "- 1.0: Identical (same word)\n",
    "- 0.8-0.9: Very similar meaning\n",
    "- 0.6-0.7: Related concepts\n",
    "- 0.4-0.5: Some relation\n",
    "- < 0.4: Not very related\n",
    "\n",
    "**Observations from the heatmap:**\n",
    "- Words with similar meanings have higher similarity scores\n",
    "- Semantic relationships are captured (e.g., king-queen, man-woman)\n",
    "- Category relationships (e.g., cat-dog, computer-phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(target_word, words, embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the most similar words to a target word.\n",
    "    \"\"\"\n",
    "    target_emb = embeddings[target_word].reshape(1, -1)\n",
    "    similarities = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word != target_word:\n",
    "            emb = embeddings[word].reshape(1, -1)\n",
    "            sim = cosine_similarity(target_emb, emb)[0, 0]\n",
    "            similarities.append((word, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nWords most similar to '{target_word}':\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (word, sim) in enumerate(similarities[:top_k], 1):\n",
    "        print(f\"{i}. {word:<15} (similarity: {sim:.4f})\")\n",
    "\n",
    "find_most_similar(\"king\", words, embeddings)\n",
    "find_most_similar(\"computer\", words, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings in 2D\n",
    "\n",
    "Embeddings exist in high-dimensional space (768 dimensions for GPT-2). We can use dimensionality reduction to visualize them in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings_2d(words, embeddings):\n",
    "    \"\"\"\n",
    "    Visualize embeddings in 2D using PCA.\n",
    "    \"\"\"\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.array([embeddings[word] for word in words])\n",
    "    \n",
    "    # Reduce to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embedding_matrix)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100, alpha=0.6)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, \n",
    "                    (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                    fontsize=12,\n",
    "                    ha='center',\n",
    "                    va='bottom')\n",
    "    \n",
    "    plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.title('Word Embeddings Visualized in 2D (PCA)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTotal variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "visualize_embeddings_2d(words, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Arithmetic: The Famous \"King - Man + Woman = Queen\" Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_arithmetic_example(embeddings, tokenizer):\n",
    "    \"\"\"\n",
    "    Demonstrate vector arithmetic with embeddings.\n",
    "    \"\"\"\n",
    "    # Get embeddings\n",
    "    king_emb = embeddings['king']\n",
    "    man_emb = embeddings['man']\n",
    "    woman_emb = embeddings['woman']\n",
    "    \n",
    "    # Compute: king - man + woman\n",
    "    result_emb = king_emb - man_emb + woman_emb\n",
    "    \n",
    "    # Find closest word to result\n",
    "    vocab_size = len(tokenizer)\n",
    "    all_embeddings = model.transformer.wte.weight.detach().numpy()\n",
    "    \n",
    "    # Compute similarities with all words (sample first 5000 for speed)\n",
    "    sample_size = min(5000, vocab_size)\n",
    "    similarities = cosine_similarity([result_emb], all_embeddings[:sample_size])[0]\n",
    "    \n",
    "    # Get top 10 matches\n",
    "    top_indices = np.argsort(similarities)[::-1][:10]\n",
    "    \n",
    "    print(\"Vector Arithmetic: king - man + woman = ?\\n\")\n",
    "    print(\"Top 10 closest words:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        word = tokenizer.decode([idx])\n",
    "        sim = similarities[idx]\n",
    "        print(f\"{i:2d}. {word:<20} (similarity: {sim:.4f})\")\n",
    "\n",
    "vector_arithmetic_example(embeddings, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring More Word Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore more semantic categories\n",
    "semantic_groups = {\n",
    "    \"Royalty\": [\"king\", \"queen\", \"prince\", \"princess\"],\n",
    "    \"Animals\": [\"cat\", \"dog\", \"lion\", \"tiger\"],\n",
    "    \"Technology\": [\"computer\", \"phone\", \"internet\", \"software\"],\n",
    "    \"Countries\": [\"France\", \"Rwanda\", \"Japan\", \"Brazil\"],\n",
    "}\n",
    "\n",
    "# Get embeddings for all words\n",
    "all_words = []\n",
    "all_embeddings = {}\n",
    "\n",
    "for category, words_list in semantic_groups.items():\n",
    "    for word in words_list:\n",
    "        try:\n",
    "            all_embeddings[word] = get_word_embedding(word, model, tokenizer)\n",
    "            all_words.append(word)\n",
    "        except:\n",
    "            print(f\"Could not get embedding for: {word}\")\n",
    "\n",
    "print(f\"\\nGot embeddings for {len(all_words)} words\")\n",
    "\n",
    "# Visualize all semantic groups\n",
    "if len(all_words) > 0:\n",
    "    visualize_embeddings_2d(all_words, all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 3: Explore Embeddings\n",
    "\n",
    "### Part A: Custom Word Lists\n",
    "\n",
    "Create your own word lists and explore their embeddings:\n",
    "\n",
    "**Suggested explorations:**\n",
    "1. Professional titles (doctor, teacher, engineer, farmer)\n",
    "2. Colors (red, blue, green, yellow)\n",
    "3. Emotions (happy, sad, angry, excited)\n",
    "4. Foods (rice, bread, banana, coffee)\n",
    "5. Kinyarwanda words (if available in tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Create your own word list\n",
    "your_words = [\n",
    "    \"doctor\", \"teacher\", \"engineer\", \"farmer\",\n",
    "    \"hospital\", \"school\", \"office\", \"farm\"\n",
    "]\n",
    "\n",
    "# Get embeddings\n",
    "your_embeddings = {}\n",
    "for word in your_words:\n",
    "    try:\n",
    "        your_embeddings[word] = get_word_embedding(word, model, tokenizer)\n",
    "    except:\n",
    "        print(f\"Skipping: {word}\")\n",
    "\n",
    "# Analyze\n",
    "if len(your_embeddings) > 1:\n",
    "    print(\"\\nSimilarity Analysis:\")\n",
    "    valid_words = list(your_embeddings.keys())\n",
    "    sim_matrix = compute_similarity_matrix(valid_words, your_embeddings)\n",
    "    visualize_similarity_matrix(valid_words, sim_matrix)\n",
    "    visualize_embeddings_2d(valid_words, your_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Vector Arithmetic Experiments\n",
    "\n",
    "Try your own vector arithmetic:\n",
    "- Paris - France + Rwanda = ?\n",
    "- Doctor - Hospital + School = ?\n",
    "- Computer - Technology + Nature = ?\n",
    "\n",
    "Think about:\n",
    "- What relationships are captured?\n",
    "- What relationships are missed?\n",
    "- Why might some analogies work better than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom vector arithmetic here\n",
    "# Example: word1 - word2 + word3 = ?\n",
    "\n",
    "def custom_vector_arithmetic(word1, word2, word3, tokenizer, model, top_k=10):\n",
    "    \"\"\"\n",
    "    Compute: word1 - word2 + word3 = ?\n",
    "    \"\"\"\n",
    "    try:\n",
    "        emb1 = get_word_embedding(word1, model, tokenizer)\n",
    "        emb2 = get_word_embedding(word2, model, tokenizer)\n",
    "        emb3 = get_word_embedding(word3, model, tokenizer)\n",
    "        \n",
    "        result = emb1 - emb2 + emb3\n",
    "        \n",
    "        # Find closest words\n",
    "        all_embeddings = model.transformer.wte.weight.detach().numpy()\n",
    "        similarities = cosine_similarity([result], all_embeddings[:5000])[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        print(f\"\\n{word1} - {word2} + {word3} = ?\\n\")\n",
    "        print(\"Top matches:\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, idx in enumerate(top_indices, 1):\n",
    "            word = tokenizer.decode([idx])\n",
    "            print(f\"{i:2d}. {word:<20} ({similarities[idx]:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Try some analogies\n",
    "custom_vector_arithmetic(\"Paris\", \"France\", \"Rwanda\", tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Key Takeaways\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "### 1. Next Token Prediction\n",
    "- LLMs predict the next token based on probability distributions\n",
    "- Temperature controls randomness in generation\n",
    "- The model assigns probabilities to thousands of possible next tokens\n",
    "\n",
    "### 2. Tokenization\n",
    "- Text is broken into tokens (words, subwords, or characters)\n",
    "- Different tokenizers handle languages differently\n",
    "- Languages with less training data are often tokenized less efficiently\n",
    "- Kinyarwanda and other low-resource languages may require more tokens\n",
    "- This affects both cost (API pricing) and model performance\n",
    "\n",
    "### 3. Vector Embeddings\n",
    "- Words are represented as vectors in high-dimensional space\n",
    "- Similar meanings have similar vectors\n",
    "- We can measure similarity using cosine similarity\n",
    "- Embeddings capture semantic relationships\n",
    "- Vector arithmetic can reveal word analogies\n",
    "\n",
    "## Important Implications\n",
    "\n",
    "### For Low-Resource Languages (like Kinyarwanda):\n",
    "1. **Tokenization Challenges**: More tokens needed ‚Üí Higher costs, longer context\n",
    "2. **Representation**: Fewer examples in training data ‚Üí Potentially less accurate\n",
    "3. **Solutions**:\n",
    "   - Train language-specific tokenizers\n",
    "   - Use multilingual models\n",
    "   - Fine-tune on local language data\n",
    "   - Develop community datasets\n",
    "\n",
    "### For Model Development:\n",
    "1. Tokenization strategy affects model performance\n",
    "2. Embeddings quality depends on training data\n",
    "3. Context length limitations impact what the model can process\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Explore More Models**: Try different open-source models (Llama, Mistral, etc.)\n",
    "2. **Build Custom Tokenizers**: Create tokenizers optimized for Kinyarwanda\n",
    "3. **Fine-tuning**: Adapt models for specific tasks or languages\n",
    "4. **Contribute**: Help build datasets for low-resource languages\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- OpenAI Tokenizer: https://platform.openai.com/tokenizer\n",
    "- Hugging Face Transformers: https://huggingface.co/transformers/\n",
    "- Papers:\n",
    "  - \"Attention Is All You Need\" (Transformer architecture)\n",
    "  - \"Language Models are Few-Shot Learners\" (GPT-3)\n",
    "  - \"Neural Machine Translation of Rare Words with Subword Units\" (BPE)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Final Exercise: Reflection Questions\n",
    "\n",
    "1. How might tokenization inefficiency affect the cost of using LLMs for Kinyarwanda applications?\n",
    "2. What are some strategies to improve LLM performance for low-resource languages?\n",
    "3. How do embeddings capture meaning, and what are their limitations?\n",
    "4. Why is understanding these fundamentals important for building AI applications?\n",
    "\n",
    "**Discussion**: Share your insights with your peers and instructor!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
