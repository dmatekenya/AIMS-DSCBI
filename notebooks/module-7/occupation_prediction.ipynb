{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupation Code Prediction: TF-IDF vs LLM Embeddings\n",
    "\n",
    "This notebook compares two text vectorization approaches for predicting occupation ISCO codes:\n",
    "1. **TF-IDF (Term Frequency-Inverse Document Frequency)**: Traditional statistical method\n",
    "2. **LLM Embeddings (Sentence Transformers)**: Deep learning-based semantic embeddings\n",
    "\n",
    "## Dataset\n",
    "- **Features**: occupation_description, industry_description, employment_sector\n",
    "- **Target**: occupation_isco_code (445 classes)\n",
    "- **Size**: 111,664 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers scikit-learn pandas numpy matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('lfs.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"Number of unique occupation codes:\", df['occupation_isco_code'].nunique())\n",
    "print(\"\\nTop 15 most common occupations:\")\n",
    "top_occupations = df['occupation_isco_code'].value_counts().head(15)\n",
    "print(top_occupations)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_occupations.plot(kind='bar', color='steelblue')\n",
    "plt.title('Top 15 Most Common Occupation Codes')\n",
    "plt.xlabel('Occupation ISCO Code')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employment sector distribution\n",
    "print(\"\\nEmployment Sector Distribution:\")\n",
    "print(df['employment_sector'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['employment_sector'].value_counts().plot(kind='bar', color='coral')\n",
    "plt.title('Employment Sector Distribution')\n",
    "plt.xlabel('Sector')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "df['occupation_length'] = df['occupation_description'].str.len()\n",
    "df['industry_length'] = df['industry_description'].str.len()\n",
    "\n",
    "print(\"\\nText Length Statistics:\")\n",
    "print(\"\\nOccupation Description:\")\n",
    "print(df['occupation_length'].describe())\n",
    "print(\"\\nIndustry Description:\")\n",
    "print(df['industry_length'].describe())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].hist(df['occupation_length'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Occupation Description Length Distribution')\n",
    "axes[0].set_xlabel('Character Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(df['industry_length'], bins=50, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_title('Industry Description Length Distribution')\n",
    "axes[1].set_xlabel('Character Length')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text features into a single column\n",
    "df['combined_text'] = (df['occupation_description'].astype(str) + ' ' + \n",
    "                       df['industry_description'].astype(str) + ' ' + \n",
    "                       df['employment_sector'].astype(str))\n",
    "\n",
    "print(\"Sample combined text:\")\n",
    "print(df['combined_text'].iloc[0])\n",
    "print(\"\\nCombined text length:\")\n",
    "print(df['combined_text'].str.len().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df['combined_text']\n",
    "y = df['occupation_isco_code']\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train):,}\")\n",
    "print(f\"Test set size: {len(X_test):,}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Approach 1: TF-IDF Vectorization\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical method that:\n",
    "- Represents text as sparse vectors based on word frequencies\n",
    "- Weights words by their importance (rare words get higher weights)\n",
    "- Fast and interpretable\n",
    "- No semantic understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"APPROACH 1: TF-IDF VECTORIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "start_time = time.time()\n",
    "print(\"\\n[1/3] Creating TF-IDF vectors...\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit to top 5000 features\n",
    "    ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "    min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.8  # Ignore terms that appear in more than 80% of documents\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "vectorization_time = time.time() - start_time\n",
    "print(f\"   ✓ Vectorization completed in {vectorization_time:.2f} seconds\")\n",
    "print(f\"   ✓ Feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"   ✓ Sparsity: {(1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model Training: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/3] Training Logistic Regression model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_tfidf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "lr_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"   ✓ Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/3] Evaluating model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "y_pred_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "f1_macro_tfidf = f1_score(y_test, y_pred_tfidf, average='macro')\n",
    "f1_weighted_tfidf = f1_score(y_test, y_pred_tfidf, average='weighted')\n",
    "\n",
    "print(f\"   ✓ Prediction completed in {prediction_time:.2f} seconds\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TF-IDF RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:          {accuracy_tfidf*100:.2f}%\")\n",
    "print(f\"F1-Score (Macro):  {f1_macro_tfidf:.4f}\")\n",
    "print(f\"F1-Score (Weighted): {f1_weighted_tfidf:.4f}\")\n",
    "print(f\"\\nTiming:\")\n",
    "print(f\"  Vectorization:   {vectorization_time:.2f}s\")\n",
    "print(f\"  Training:        {training_time:.2f}s\")\n",
    "print(f\"  Prediction:      {prediction_time:.2f}s\")\n",
    "print(f\"  Total:           {vectorization_time + training_time + prediction_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Alternative: Random Forest with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Random Forest with TF-IDF...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_tfidf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "rf_training_time = time.time() - start_time\n",
    "y_pred_rf_tfidf = rf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_rf_tfidf = accuracy_score(y_test, y_pred_rf_tfidf)\n",
    "f1_weighted_rf_tfidf = f1_score(y_test, y_pred_rf_tfidf, average='weighted')\n",
    "\n",
    "print(f\"\\nRandom Forest + TF-IDF Results:\")\n",
    "print(f\"Accuracy:          {accuracy_rf_tfidf*100:.2f}%\")\n",
    "print(f\"F1-Score (Weighted): {f1_weighted_rf_tfidf:.4f}\")\n",
    "print(f\"Training Time:     {rf_training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Approach 2: LLM Embeddings (Sentence Transformers)\n",
    "\n",
    "Sentence Transformers create dense semantic embeddings that:\n",
    "- Capture semantic meaning and context\n",
    "- Generate fixed-size dense vectors\n",
    "- Pre-trained on large corpora\n",
    "- Better generalization but slower and more resource-intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"APPROACH 2: LLM EMBEDDINGS (SENTENCE TRANSFORMERS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load pre-trained model\n",
    "print(\"\\n[1/4] Loading Sentence Transformer model...\")\n",
    "# Using a lightweight multilingual model\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(\"   ✓ Model loaded successfully\")\n",
    "print(f\"   ✓ Model: paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(f\"   ✓ Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/4] Generating embeddings for training set...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate embeddings (with progress bar)\n",
    "X_train_embeddings = embedding_model.encode(\n",
    "    X_train.tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "train_embedding_time = time.time() - start_time\n",
    "print(f\"   ✓ Training embeddings generated in {train_embedding_time:.2f} seconds\")\n",
    "print(f\"   ✓ Shape: {X_train_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/4] Generating embeddings for test set...\")\n",
    "start_time = time.time()\n",
    "\n",
    "X_test_embeddings = embedding_model.encode(\n",
    "    X_test.tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "test_embedding_time = time.time() - start_time\n",
    "print(f\"   ✓ Test embeddings generated in {test_embedding_time:.2f} seconds\")\n",
    "print(f\"   ✓ Shape: {X_test_embeddings.shape}\")\n",
    "\n",
    "total_embedding_time = train_embedding_time + test_embedding_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model Training: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4/4] Training Logistic Regression model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_embeddings = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "lr_embeddings.fit(X_train_embeddings, y_train)\n",
    "\n",
    "training_time_emb = time.time() - start_time\n",
    "print(f\"   ✓ Training completed in {training_time_emb:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "y_pred_embeddings = lr_embeddings.predict(X_test_embeddings)\n",
    "\n",
    "prediction_time_emb = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_embeddings = accuracy_score(y_test, y_pred_embeddings)\n",
    "f1_macro_embeddings = f1_score(y_test, y_pred_embeddings, average='macro')\n",
    "f1_weighted_embeddings = f1_score(y_test, y_pred_embeddings, average='weighted')\n",
    "\n",
    "print(f\"   ✓ Prediction completed in {prediction_time_emb:.2f} seconds\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LLM EMBEDDINGS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:          {accuracy_embeddings*100:.2f}%\")\n",
    "print(f\"F1-Score (Macro):  {f1_macro_embeddings:.4f}\")\n",
    "print(f\"F1-Score (Weighted): {f1_weighted_embeddings:.4f}\")\n",
    "print(f\"\\nTiming:\")\n",
    "print(f\"  Embedding Generation: {total_embedding_time:.2f}s\")\n",
    "print(f\"  Training:        {training_time_emb:.2f}s\")\n",
    "print(f\"  Prediction:      {prediction_time_emb:.2f}s\")\n",
    "print(f\"  Total:           {total_embedding_time + training_time_emb + prediction_time_emb:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Alternative: Random Forest with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Random Forest with Embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_embeddings = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf_embeddings.fit(X_train_embeddings, y_train)\n",
    "\n",
    "rf_training_time_emb = time.time() - start_time\n",
    "y_pred_rf_embeddings = rf_embeddings.predict(X_test_embeddings)\n",
    "\n",
    "accuracy_rf_embeddings = accuracy_score(y_test, y_pred_rf_embeddings)\n",
    "f1_weighted_rf_embeddings = f1_score(y_test, y_pred_rf_embeddings, average='weighted')\n",
    "\n",
    "print(f\"\\nRandom Forest + Embeddings Results:\")\n",
    "print(f\"Accuracy:          {accuracy_rf_embeddings*100:.2f}%\")\n",
    "print(f\"F1-Score (Weighted): {f1_weighted_rf_embeddings:.4f}\")\n",
    "print(f\"Training Time:     {rf_training_time_emb:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['TF-IDF + LR', 'TF-IDF + RF', 'Embeddings + LR', 'Embeddings + RF'],\n",
    "    'Accuracy': [\n",
    "        accuracy_tfidf * 100,\n",
    "        accuracy_rf_tfidf * 100,\n",
    "        accuracy_embeddings * 100,\n",
    "        accuracy_rf_embeddings * 100\n",
    "    ],\n",
    "    'F1 (Weighted)': [\n",
    "        f1_weighted_tfidf,\n",
    "        f1_weighted_rf_tfidf,\n",
    "        f1_weighted_embeddings,\n",
    "        f1_weighted_rf_embeddings\n",
    "    ],\n",
    "    'Total Time (s)': [\n",
    "        vectorization_time + training_time + prediction_time,\n",
    "        vectorization_time + rf_training_time,\n",
    "        total_embedding_time + training_time_emb + prediction_time_emb,\n",
    "        total_embedding_time + rf_training_time_emb\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(comparison['Method'], comparison['Accuracy'], color=['steelblue', 'skyblue', 'coral', 'lightsalmon'])\n",
    "axes[0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_ylim([0, 100])\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison['Accuracy']):\n",
    "    axes[0].text(i, v + 1, f'{v:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# F1 Score comparison\n",
    "axes[1].bar(comparison['Method'], comparison['F1 (Weighted)'], color=['steelblue', 'skyblue', 'coral', 'lightsalmon'])\n",
    "axes[1].set_title('F1 Score (Weighted) Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison['F1 (Weighted)']):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Time comparison\n",
    "axes[2].bar(comparison['Method'], comparison['Total Time (s)'], color=['steelblue', 'skyblue', 'coral', 'lightsalmon'])\n",
    "axes[2].set_title('Total Time Comparison', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Time (seconds)')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison['Total Time (s)']):\n",
    "    axes[2].text(i, v + max(comparison['Total Time (s)'])*0.02, f'{v:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification report for best performing model\n",
    "best_model_name = comparison.loc[comparison['Accuracy'].idxmax(), 'Method']\n",
    "print(f\"\\nDetailed Classification Report for Best Model: {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'Embeddings' in best_model_name:\n",
    "    if 'RF' in best_model_name:\n",
    "        y_pred_best = y_pred_rf_embeddings\n",
    "    else:\n",
    "        y_pred_best = y_pred_embeddings\n",
    "else:\n",
    "    if 'RF' in best_model_name:\n",
    "        y_pred_best = y_pred_rf_tfidf\n",
    "    else:\n",
    "        y_pred_best = y_pred_tfidf\n",
    "\n",
    "print(classification_report(y_test, y_pred_best, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy for top 10 classes\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "top_10_classes = y_train.value_counts().head(10).index\n",
    "\n",
    "class_accuracies_tfidf = []\n",
    "class_accuracies_emb = []\n",
    "\n",
    "for cls in top_10_classes:\n",
    "    mask = y_test == cls\n",
    "    if mask.sum() > 0:\n",
    "        acc_tfidf = accuracy_score(y_test[mask], y_pred_tfidf[mask])\n",
    "        acc_emb = accuracy_score(y_test[mask], y_pred_embeddings[mask])\n",
    "        class_accuracies_tfidf.append(acc_tfidf * 100)\n",
    "        class_accuracies_emb.append(acc_emb * 100)\n",
    "\n",
    "# Plot per-class accuracy\n",
    "x = np.arange(len(top_10_classes))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars1 = ax.bar(x - width/2, class_accuracies_tfidf, width, label='TF-IDF + LR', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, class_accuracies_emb, width, label='Embeddings + LR', color='coral')\n",
    "\n",
    "ax.set_xlabel('Occupation Code')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Per-Class Accuracy for Top 10 Most Common Occupations')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{int(c)}' for c in top_10_classes], rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions\n",
    "comparison_df = pd.DataFrame({\n",
    "    'text': X_test.values,\n",
    "    'true_label': y_test.values,\n",
    "    'tfidf_pred': y_pred_tfidf,\n",
    "    'embedding_pred': y_pred_embeddings\n",
    "})\n",
    "\n",
    "# Cases where embeddings got it right but TF-IDF didn't\n",
    "embeddings_better = comparison_df[\n",
    "    (comparison_df['embedding_pred'] == comparison_df['true_label']) &\n",
    "    (comparison_df['tfidf_pred'] != comparison_df['true_label'])\n",
    "]\n",
    "\n",
    "# Cases where TF-IDF got it right but embeddings didn't\n",
    "tfidf_better = comparison_df[\n",
    "    (comparison_df['tfidf_pred'] == comparison_df['true_label']) &\n",
    "    (comparison_df['embedding_pred'] != comparison_df['true_label'])\n",
    "]\n",
    "\n",
    "print(f\"\\nCases where Embeddings outperformed TF-IDF: {len(embeddings_better)}\")\n",
    "print(f\"Cases where TF-IDF outperformed Embeddings: {len(tfidf_better)}\")\n",
    "print(f\"\\nExamples where Embeddings performed better:\")\n",
    "print(embeddings_better[['text', 'true_label', 'tfidf_pred', 'embedding_pred']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Recommendations\n",
    "\n",
    "### Summary\n",
    "\n",
    "#### TF-IDF Approach:\n",
    "**Pros:**\n",
    "- Fast training and inference\n",
    "- Low computational requirements\n",
    "- Interpretable (can see which words are important)\n",
    "- Good baseline performance\n",
    "\n",
    "**Cons:**\n",
    "- No semantic understanding\n",
    "- Sensitive to vocabulary and spelling variations\n",
    "- Struggles with synonyms and paraphrases\n",
    "- High-dimensional sparse vectors\n",
    "\n",
    "#### LLM Embeddings Approach:\n",
    "**Pros:**\n",
    "- Captures semantic meaning\n",
    "- Better generalization to unseen text\n",
    "- Robust to spelling variations and synonyms\n",
    "- Dense, fixed-size representations\n",
    "- Pre-trained on large multilingual corpora\n",
    "\n",
    "**Cons:**\n",
    "- Slower (especially embedding generation)\n",
    "- Higher computational requirements\n",
    "- Less interpretable\n",
    "- Requires more memory\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **For Production Systems**: \n",
    "   - Use **TF-IDF** if speed and computational efficiency are critical\n",
    "   - Use **LLM Embeddings** if accuracy and semantic understanding are more important\n",
    "\n",
    "2. **Hybrid Approach**:\n",
    "   - Consider combining both methods (ensemble)\n",
    "   - Use TF-IDF for initial filtering, then embeddings for refinement\n",
    "\n",
    "3. **For Kinyarwanda Data**:\n",
    "   - LLM embeddings may perform better due to multilingual pre-training\n",
    "   - Consider fine-tuning embedding models on Kinyarwanda text\n",
    "   - Explore African language-specific models (AfroXLMR)\n",
    "\n",
    "4. **Future Improvements**:\n",
    "   - Try different embedding models (larger models, domain-specific)\n",
    "   - Experiment with advanced classifiers (XGBoost, Neural Networks)\n",
    "   - Use data augmentation for rare classes\n",
    "   - Implement class balancing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best performing model\n",
    "best_idx = comparison['Accuracy'].idxmax()\n",
    "best_method = comparison.loc[best_idx, 'Method']\n",
    "\n",
    "print(f\"\\nSaving best model: {best_method}\")\n",
    "\n",
    "if 'Embeddings' in best_method:\n",
    "    if 'RF' in best_method:\n",
    "        model_to_save = rf_embeddings\n",
    "        filename = 'best_model_embeddings_rf.pkl'\n",
    "    else:\n",
    "        model_to_save = lr_embeddings\n",
    "        filename = 'best_model_embeddings_lr.pkl'\n",
    "else:\n",
    "    if 'RF' in best_method:\n",
    "        model_to_save = rf_tfidf\n",
    "        filename = 'best_model_tfidf_rf.pkl'\n",
    "        # Also save vectorizer\n",
    "        with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "            pickle.dump(tfidf, f)\n",
    "        print(\"   ✓ TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'\")\n",
    "    else:\n",
    "        model_to_save = lr_tfidf\n",
    "        filename = 'best_model_tfidf_lr.pkl'\n",
    "        # Also save vectorizer\n",
    "        with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "            pickle.dump(tfidf, f)\n",
    "        print(\"   ✓ TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'\")\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(model_to_save, f)\n",
    "\n",
    "print(f\"   ✓ Best model saved as '{filename}'\")\n",
    "print(f\"\\nTo use the model:\")\n",
    "print(f\"```python\")\n",
    "print(f\"import pickle\")\n",
    "print(f\"with open('{filename}', 'rb') as f:\")\n",
    "print(f\"    model = pickle.load(f)\")\n",
    "if 'tfidf' in filename.lower():\n",
    "    print(f\"with open('tfidf_vectorizer.pkl', 'rb') as f:\")\n",
    "    print(f\"    vectorizer = pickle.load(f)\")\n",
    "print(f\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prediction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "sample_texts = [\n",
    "    \"UMUSHOFERI WA HULUX ENTREPRISE Y'UBWUBATSI,GUTANGA UMURIRO N'AMAZI Private\",\n",
    "    \"COMPTABLE BACURUZA IBIKORESHO BYA MUDASOMBWA NIBYITUMANAHO Private\",\n",
    "    \"UMUKOZI WO MURUGO IMIRIMO YO MURUGO Household\"\n",
    "]\n",
    "\n",
    "print(\"\\nSample Predictions:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    # TF-IDF prediction\n",
    "    text_tfidf = tfidf.transform([text])\n",
    "    pred_tfidf = lr_tfidf.predict(text_tfidf)[0]\n",
    "    \n",
    "    # Embedding prediction\n",
    "    text_embedding = embedding_model.encode([text])\n",
    "    pred_embedding = lr_embeddings.predict(text_embedding)[0]\n",
    "    \n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"TF-IDF Prediction:    {pred_tfidf}\")\n",
    "    print(f\"Embedding Prediction: {pred_embedding}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
