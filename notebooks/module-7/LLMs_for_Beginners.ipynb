{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f0d8db",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŒŸ LLMs for Beginners â€” Hands-on Notebook\n",
    "\n",
    "This notebook gives you **simple, concrete demos** of how large language models (LLMs) work:\n",
    "1. **Next-token prediction** (how LLMs generate text)  \n",
    "2. **Tokenization** (splitting text into tokens)  \n",
    "3. **Vector embeddings** (turning text into vectors for similarity & search)  \n",
    "\n",
    "> Designed to run with minimal dependencies. Optional cells use `transformers` and `sentence-transformers` if you have them installed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3f633",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Setup\n",
    "This notebook works **out of the box** using only `numpy` and `scikit-learn` (for a basic embeddings demo).  \n",
    "If you want to try **real LLM tokenizers/embeddings**, install:\n",
    "```bash\n",
    "pip install transformers sentence-transformers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35dad0",
   "metadata": {},
   "source": [
    "\n",
    "## 1) ðŸ“– Next-token prediction (the core idea)\n",
    "\n",
    "LLMs generate text by predicting the **next token** given previous tokens.  \n",
    "Below is a **toy language model** to illustrate the idea without any external libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b607fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Tiny vocabulary + conditional next-token probabilities (toy)\n",
    "vocab = [\"I\", \"love\", \"like\", \"cats\", \"dogs\", \".\", \"and\"]\n",
    "probs = {\n",
    "    \"START\": {\"I\": 1.0},\n",
    "    \"I\": {\"love\": 0.6, \"like\": 0.4},\n",
    "    \"love\": {\"cats\": 0.5, \"dogs\": 0.5},\n",
    "    \"like\": {\"cats\": 0.7, \"dogs\": 0.3},\n",
    "    \"cats\": {\"and\": 0.3, \".\": 0.7},\n",
    "    \"dogs\": {\"and\": 0.3, \".\": 0.7},\n",
    "    \"and\": {\"cats\": 0.5, \"dogs\": 0.5},\n",
    "    \".\": {}\n",
    "}\n",
    "\n",
    "def sample_next(prev):\n",
    "    dist = probs.get(prev, {})\n",
    "    if not dist:\n",
    "        return \".\"\n",
    "    tokens = list(dist.keys())\n",
    "    p = np.array([dist[t] for t in tokens], dtype=float)\n",
    "    p = p / p.sum()\n",
    "    return rng.choice(tokens, p=p)\n",
    "\n",
    "def generate(max_tokens=8):\n",
    "    seq = [\"I\"]  # start tokenized as \"I\" from our toy START\n",
    "    while len(seq) < max_tokens:\n",
    "        nxt = sample_next(seq[-1])\n",
    "        seq.append(nxt)\n",
    "        if nxt == \".\":\n",
    "            break\n",
    "    return \" \".join(seq)\n",
    "\n",
    "for _ in range(3):\n",
    "    print(generate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19b241",
   "metadata": {},
   "source": [
    "\n",
    "### (Optional) Real model generation with ðŸ¤— Transformers\n",
    "If you have `transformers` installed, try a small text generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae354f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    out = gen(\"Rwanda is known for\", max_new_tokens=20, do_sample=True, top_p=0.9)\n",
    "    print(out[0][\"generated_text\"])\n",
    "except Exception as e:\n",
    "    print(\"Skipping transformers demo (install transformers, or model download failed). Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146c03e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) âœ‚ï¸ Tokenization (turning text â†’ tokens)\n",
    "\n",
    "Tokenizers split text into **tokens** (words, subwords, or characters).  \n",
    "Modern LLMs use **subword tokenization** (BPE, WordPiece) to handle rare words efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"Transformers make language tasks easier in Rwanda.\"\n",
    "print(\"Raw text:\", text)\n",
    "\n",
    "# Simple whitespace tokenizer (concept demo)\n",
    "tokens_ws = text.split()\n",
    "print(\"Whitespace tokens:\", tokens_ws)\n",
    "print(\"Number of tokens (whitespace):\", len(tokens_ws))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optional) Use a real tokenizer with ðŸ¤— Transformers if available\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    ids = tok.encode(text, add_special_tokens=True)\n",
    "    print(\"Real tokenizer IDs:\", ids)\n",
    "    print(\"Decoded back:\", tok.decode(ids))\n",
    "    print(\"Number of tokens (subword):\", len(ids))\n",
    "except Exception as e:\n",
    "    print(\"Skipping real tokenizer demo. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc5577",
   "metadata": {},
   "source": [
    "\n",
    "## 3) ðŸ”¢ Vector Embeddings (text â†’ vectors)\n",
    "\n",
    "Embeddings map text to **vectors** so we can compute **similarity**.  \n",
    "Weâ€™ll show two paths:\n",
    "1. **TF-IDF** (classic, sparse vectors) â€“ no extra installs.\n",
    "2. **Sentence embeddings** (dense, semantic vectors) â€“ needs `sentence-transformers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "docs = [\n",
    "    \"Kigali hosts data science workshops.\",\n",
    "    \"We teach beginners about machine learning.\",\n",
    "    \"Rwanda has fast-growing tech communities.\",\n",
    "    \"Neural networks power modern AI.\"\n",
    "]\n",
    "query = \"Intro workshop on machine learning in Kigali\"\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(docs + [query])\n",
    "sims = cosine_similarity(X[-1], X[:-1]).ravel()\n",
    "\n",
    "for doc, s in sorted(zip(docs, sims), key=lambda x: -x[1]):\n",
    "    print(f\"{s:.3f} :: {doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8880aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optional) Semantic embeddings with sentence-transformers\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import numpy as np\n",
    "\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    emb = model.encode(docs + [query], normalize_embeddings=True)\n",
    "    sims = emb[-1] @ emb[:-1].T  # cosine since normalized\n",
    "\n",
    "    for doc, s in sorted(zip(docs, sims), key=lambda x: -x[1]):\n",
    "        print(f\"{s:.3f} :: {doc}\")\n",
    "except Exception as e:\n",
    "    print(\"Skipping sentence-transformers demo. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e6fc6",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ§ª Mini Exercise â€” Build a tiny vector search\n",
    "- Replace the `docs` list with 6â€“10 short FAQs from your domain.  \n",
    "- Keep `query` as a user question.  \n",
    "- Run the **TF-IDF** cell to see which doc matches best.  \n",
    "- (Optional) Try **sentence-transformers** for better semantic matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf11a7",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“ˆ Visual intuition (optional)\n",
    "\n",
    "If you have `matplotlib` and `sklearn` installed, you can project embeddings to 2D with PCA to **see** how similar texts cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bdea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    # Use TF-IDF vectors for a quick plot\n",
    "    X = vec.fit_transform(docs).toarray()\n",
    "    pts = PCA(n_components=2).fit_transform(X)\n",
    "    plt.figure()\n",
    "    plt.scatter(pts[:,0], pts[:,1])\n",
    "    for i, d in enumerate(docs):\n",
    "        plt.annotate(str(i+1), (pts[i,0], pts[i,1]))\n",
    "    plt.title(\"Docs in 2D (TF-IDF + PCA)\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Skipping visualization. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3d69c",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Wrap-up\n",
    "- **LLMs** predict the **next token**, one step at a time.  \n",
    "- **Tokenization** turns text into model-friendly token IDs (often subwords).  \n",
    "- **Embeddings** map text to vectors so we can compute **similarity** and do **search**.\n",
    "\n",
    "**Where to go next**\n",
    "- Try Hugging Face pipelines for summarization, Q&A, and text generation.  \n",
    "- Swap in your **own documents** to build a tiny **semantic search** demo.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}